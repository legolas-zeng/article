{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次使用全量的训练集样本来更新模型参数，即：  $\\theta = \\theta- \\eta \\cdot \\bigtriangledown \\theta_{J}(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_epochs):\n",
    "    params_grad  = evaluate_gradient(loss_function, data, params)\n",
    "    params = params - learning_rate * params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优点: \n",
    "    - 由于每一步都利用了训练集中的所有数据,因此当损失函数达到最小值以后,能够保证此时计算出的梯度为0,换句话说,就是能够收敛.因此,使用BGD时不需要逐渐减小学习率\n",
    "- 缺点: \n",
    "    - 由于每一步都要使用所有数据,因此随着数据集的增大,运行速度会越来越慢."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机梯度下降算法每次从训练集中随机选择一个样本来进行学习，即： $\\theta = \\theta- \\eta \\cdot \\bigtriangledown \\theta_{J}(\\theta;x_{i};y_{i})$\n",
    "\n",
    "- 对比：\n",
    "    - 批量梯度下降算法：每次使用全部训练样本，速度慢；\n",
    "    - 随机梯度下降算法：每次只随机选择一个样本来更新模型参数，速度快，可在线更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for example in data:\n",
    "        params_grad = evaluate_gradient(loss_function,example,params)\n",
    "        params = params - learning_rate * params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优点: \n",
    "    - 训练速度快,对于很大的数据集,也能够以较快的速度收敛.\n",
    "\n",
    "- 缺点: \n",
    "    - 由于是抽取,因此不可避免的,得到的梯度肯定有误差.因此学习速率需要逐渐减小.否则模型无法收敛。因为误差,所以每一次迭代的梯度受抽样的影响比较大,也就是说梯度含有比较大的噪声,不能很好的反映真实梯度."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小批量梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Mini-batch 梯度下降综合了 batch 梯度下降与 stochastic 梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择 m,m<n 个样本进行学习，即：$\\theta = \\theta- \\eta \\cdot \\bigtriangledown \\theta_{J}(\\theta;x_{i}:i+m;y_{i}:i+m)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(nb_epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for batch in get_batches(data, batch_size = 50):\n",
    "        params_grad = evaluate_gradient(loss_function,batch,params)\n",
    "        params = params - learning_rate * params_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 梯度下降的目前存在的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 选择一个合理的学习率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。\n",
    "\n",
    "- 更新每次的学习速率方式很难。学习速率调整可以在每次更新过程中改变学习速率，如退火方法。一般在每次迭代中衰减一个较小的阈值或者其他调整方式。无论哪种调整方法，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点\n",
    "\n",
    "- 模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的或者每个特征有着不同的数据分布的特征和空间，那么便不能在每次更新中每个参数使用相同的学习速率。\n",
    "\n",
    "- 对于非凸目标函数，容易陷入那些次优的局部极值点中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 梯度下降优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
