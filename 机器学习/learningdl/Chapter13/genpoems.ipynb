{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动生成古诗词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集处理\n",
    "- 文字转换为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_poetry(self):  \n",
    "    with open(self.poetry_file, \"r\", encoding='utf-8') as f:  \n",
    "        poetry_list = [line for line in f]  \n",
    "    return poetry_list  \n",
    "  \n",
    "def _gen_poetry_vectors(self):  \n",
    "    words = sorted(set(''.join(self.poetry_list)+' '))  \n",
    "    # 每一个字符分配一个索引 为后续诗词向量化做准备  \n",
    "    int_to_word = {i: word for i, word in enumerate(words)}  \n",
    "    word_to_int = {v: k for k, v in int_to_word.items()}  \n",
    "    to_int = lambda word: word_to_int.get(word)  \n",
    "    poetry_vectors = [list(map(to_int, poetry)) for poetry in self.poetry_list]  \n",
    "    return poetry_vectors, word_to_int, int_to_word  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(self):  \n",
    "    start = 0  \n",
    "    end = self.batch_size  \n",
    "    for _ in range(self.chunk_size):  \n",
    "        batches = self.poetry_vectors[start:end]  \n",
    "        # 输入数据 按每块数据中诗句最大长度初始化数组，缺失数据补全  \n",
    "        x_batch = np.full((self.batch_size, max(map(len, batches))), self.word_to_int[' '], np.int32)  \n",
    "        for row in range(self.batch_size): \n",
    "            x_batch[row, :len(batches[row])] = batches[row]  \n",
    "            # 标签数据 根据上一个字符预测下一个字符 所以这里y_batch数据应为x_batch数据向后移一位  \n",
    "            y_batch = np.copy(x_batch)  \n",
    "            y_batch[:, :-1], y_batch[:, -1] = x_batch[:, 1:], x_batch[:, 0]  \n",
    "            yield x_batch, y_batch  \n",
    "            start += self.batch_size  \n",
    "            end += self.batch_size  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建RNN模型\n",
    "- 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PoetryModel:  \n",
    "  \n",
    "    def __init__(self):  \n",
    "        # 诗歌生成  \n",
    "        self.poetry = Poetry()  \n",
    "        # 单个cell训练序列个数  \n",
    "        self.batch_size = self.poetry.batch_size  \n",
    "        # 所有出现字符的数量  \n",
    "        self.word_len = len(self.poetry.word_to_int)  \n",
    "        # 隐层的数量  \n",
    "        self.rnn_size = 128  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义输入输出变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入句子长短不一致 用None自适应  \n",
    "inputs = tf.placeholder(tf.int32, shape=(self.batch_size, None), name='inputs')  \n",
    "# 输出为预测某个字后续字符 故输出也不一致  \n",
    "targets = tf.placeholder(tf.int32, shape=(self.batch_size, None), name='targets')  \n",
    "# 防止过拟合  \n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_variable(inputs, rnn_size, word_len):  \n",
    "    with tf.variable_scope('embedding'):  \n",
    "        # 这里选择使用cpu进行embedding  \n",
    "        with tf.device(\"/cpu:0\"):  \n",
    "            # 默认使用'glorot_uniform_initializer'初始化，来自源码说明:  \n",
    "            # If initializer is `None` (the default), the default initializer passed in  \n",
    "            # the variable scope will be used. If that one is `None` too, a  \n",
    "            # `glorot_uniform_initializer` will be used.  \n",
    "            # 这里实际上是根据字符数量分别生成state_size长度的向量  \n",
    "            embedding = tf.get_variable('embedding', [word_len, rnn_size])  \n",
    "            # 根据inputs序列中每一个字符对应索引 在embedding中寻找对应向量,即字符转为连续向量:[字]==>[1]==>[0,1,0]  \n",
    "            lstm_inputs = tf.nn.embedding_lookup(embedding, inputs)  \n",
    "    return lstm_inputs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义模型的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_graph(self, batch_size, rnn_size, word_len, lstm_inputs, keep_prob):  \n",
    "    # cell.state_size ==> 128  \n",
    "    # 基础cell 也可以选择其他基本cell类型  \n",
    "    lstm = tf.nn.rnn_cell.BasicLSTMCell(num_units=rnn_size)  \n",
    "    drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)  \n",
    "    # 多层cell 前一层cell作为后一层cell的输入  \n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([drop] * 2)  \n",
    "    # 初始状态生成(h0) 默认为0  \n",
    "    # initial_state.shape ==> (64, 128)  \n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)  \n",
    "    # 使用dynamic_rnn自动进行时间维度推进 且 可以使用不同长度的时间维度  \n",
    "    # 因为我们使用的句子长度不一致  \n",
    "    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, lstm_inputs, initial_state=initial_state)  \n",
    "    seq_output = tf.concat(lstm_outputs, 1)  \n",
    "    x = tf.reshape(seq_output, [-1, rnn_size])  \n",
    "    # softmax计算概率  \n",
    "    w, b = self.soft_max_variable(rnn_size, word_len)  \n",
    "    logits = tf.matmul(x, w) + b  \n",
    "    prediction = tf.nn.softmax(logits, name='predictions')  \n",
    "    return logits, prediction, initial_state, final_state  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义权重和偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def soft_max_variable(rnn_size, word_len):  \n",
    "    # 共享变量  \n",
    "    with tf.variable_scope('soft_max'):  \n",
    "        w = tf.get_variable(\"w\", [rnn_size, word_len])  \n",
    "        b = tf.get_variable(\"b\", [word_len])  \n",
    "    return w, b  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@staticmethod  \n",
    "def loss_graph(word_len, targets, logits):  \n",
    "    # 将y序列按序列值转为one_hot向量  \n",
    "    y_one_hot = tf.one_hot(targets, word_len)  \n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, word_len])  \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))  \n",
    "    return loss  \n",
    " \n",
    "@staticmethod  \n",
    "def optimizer_graph(loss, learning_rate):  \n",
    "    grad_clip = 5  \n",
    "    # 使用clipping gradients  \n",
    "    tvars = tf.trainable_variables()  \n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)  \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)  \n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))  \n",
    "    return optimizer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始训练  \n",
    "saver = tf.train.Saver()  \n",
    "sess = tf.Session()  \n",
    "sess.run(tf.global_variables_initializer())  \n",
    "step = 0  \n",
    "new_state = sess.run(initial_state)  \n",
    "for i in range(epoch):  \n",
    "    # 训练数据生成器  \n",
    "    batches = self.poetry.batch()  \n",
    "    # 随模型进行训练 降低学习率  \n",
    "    sess.run(tf.assign(learning_rate, 0.001 * (0.97 ** i)))  \n",
    "    for batch_x, batch_y in batches:  \n",
    "        feed = {inputs: batch_x, targets: batch_y, initial_state: new_state, keep_prob: 0.5}  \n",
    "        batch_loss, _, new_state = sess.run([loss, optimizer, final_state], feed_dict=feed)  \n",
    "        print(datetime.datetime.now().strftime('%c'), ' i:', i, 'step:', step, ' batch_loss:', batch_loss)  \n",
    "        step += 1  \n",
    "model_path = os.getcwd() + os.sep + \"poetry.model\"  \n",
    "saver.save(sess, model_path, global_step=step)  \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成古诗词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen(self, poem_len):  \n",
    "    def to_word(weights):  \n",
    "        t = np.cumsum(weights)  \n",
    "        s = np.sum(weights)  \n",
    "        sample = int(np.searchsorted(t, np.random.rand(1) * s))  \n",
    "        return self.poetry.int_to_word[sample]  \n",
    "  \n",
    "    # 输入  \n",
    "    # 句子长短不一致 用None自适应  \n",
    "    self.batch_size = 1  \n",
    "    inputs = tf.placeholder(tf.int32, shape=(self.batch_size, 1), name='inputs')  \n",
    "    # 防止过拟合  \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')  \n",
    "    lstm_inputs = self.embedding_variable(inputs, self.rnn_size, self.word_len)  \n",
    "    # rnn模型  \n",
    "    _, prediction, initial_state, final_state = self.rnn_graph(self.batch_size, self.rnn_size, self.word_len, lstm_inputs, keep_prob)  \n",
    "  \n",
    "    saver = tf.train.Saver()  \n",
    "    with tf.Session() as sess:  \n",
    "        sess.run(tf.global_variables_initializer())  \n",
    "        saver.restore(sess, tf.train.latest_checkpoint('.'))  \n",
    "        new_state = sess.run(initial_state)  \n",
    "  \n",
    "        # 在所有字中随机选择一个作为开始  \n",
    "        x = np.zeros((1, 1))  \n",
    "        x[0, 0] = self.poetry.word_to_int[self.poetry.int_to_word[random.randint(1, self.word_len-1)]]  \n",
    "        feed = {inputs: x, initial_state: new_state, keep_prob: 1}  \n",
    "  \n",
    "        predict, new_state = sess.run([prediction, final_state], feed_dict=feed)  \n",
    "        word = to_word(predict)  \n",
    "        poem = ''  \n",
    "        while len(poem) < poem_len:  \n",
    "            poem += word  \n",
    "            x = np.zeros((1, 1))  \n",
    "            x[0, 0] = self.poetry.word_to_int[word]  \n",
    "            feed = {inputs: x, initial_state: new_state, keep_prob: 1}  \n",
    "            predict, new_state = sess.run([prediction, final_state], feed_dict=feed)  \n",
    "            word = to_word(predict)  \n",
    "        return poem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结果展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "叶下寒泉劲，\n",
    "龙衣万网初。\n",
    "步行鱼袖里，\n",
    "掩弄恨燕图。\n",
    "\n",
    "歌对半门风雨晓，\n",
    "五阳霜事绿金铺。\n",
    "不愁睡起花三点，\n",
    "一叶兔庐愁自秃。\n",
    "\n",
    "雨洒寒空堕瓦盘，\n",
    "觅鸦无火照林间。\n",
    "西江水暗田难醉，\n",
    "正为邻王压口看。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
