{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教你做一个简单的聊天机器人！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的工作原理\n",
    "三个模块：\n",
    "- 提问处理模块:\n",
    "    - 查询关键词生成、答案类型确定、句法和语义分析\n",
    "- 检索模块:\n",
    "    - 根据查询关键词所信息检索，返回句子或段落\n",
    "- 答案抽取模块:\n",
    "    - 通过分析和推理从检索出的句子或段落里抽取出和提问一致的实体，再根据概率最大对候选答案排序,最后选出最终的回答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的关键技术\n",
    "- 海量文本知识表示\n",
    "    - 网络文本资源获取、机器学习方法、大规模语义计算和推理、知识表示体系、知识库构建；\n",
    "\n",
    "- 问句解析\n",
    "    - 中文分词、词性标注、实体标注、概念类别标注、句法分析、语义分析、逻辑结构标注、指代消解、关联关系标注、问句分类（简单问句还是复杂问句、实体型还是段落型还是篇章级问题）、答案类别确定；\n",
    "\n",
    "- 答案生成与过滤\n",
    "    - 候选答案抽取、关系推演（并列关系还是递进关系还是因果关系）、吻合程度判断、噪声过滤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的技术方法\n",
    "- 基于检索的技术\n",
    "- 基于模式匹配的技术\n",
    "- 基于自然语言理解的技术\n",
    "- 基于统计翻译模型的技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的常用模型\n",
    "- 循环神经网络和LSTM\n",
    "- seq2seq\n",
    "- attention模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow的seq2seq模型api\n",
    "embedding_attention_seq2seq(\n",
    "    encoder_inputs,\n",
    "    decoder_inputs,\n",
    "    cell,\n",
    "    num_encoder_symbols,\n",
    "    num_decoder_symbols,\n",
    "    embedding_size,\n",
    "    num_heads=1,\n",
    "    output_projection=None,\n",
    "    feed_previous=False,\n",
    "    dtype=None,\n",
    "    scope=None,\n",
    "    initial_state_attention=False\n",
    ") \n",
    "\n",
    "\n",
    "# 参数encoder_inputs是一个list，list中每一项是1D的Tensor，\n",
    "# 这个Tensor的shape是[batch_size]，Tensor中每一项是一个整数，类似这样：\n",
    "[array([0, 0, 0, 0], dtype=int32), \n",
    "array([0, 0, 0, 0], dtype=int32), \n",
    "array([8, 3, 5, 3], dtype=int32), \n",
    "array([7, 8, 2, 1], dtype=int32), \n",
    "array([6, 2, 10, 9], dtype=int32)]\n",
    "\n",
    "\n",
    "# 它的返回值是一个(outputs, state)结构的tuple，\n",
    "# 其中outputs是一个长度为句子长度(词数，与上面encoder_inputs的list长度一样)的list，\n",
    "# list中每一项是一个2D的tf.float32类型的Tensor，\n",
    "# 第一维度是样本数，比如4个样本则有四组Tensor，每个Tensor长度是embedding_size，像下面的样子：\n",
    "[array([\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143],\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143],\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143],\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143]\n",
    "    ],dtype=float32),\n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入序列长度\n",
    "input_seq_len = 5\n",
    "# 输出序列长度\n",
    "output_seq_len = 5\n",
    "# 空值填充0\n",
    "PAD_ID = 0\n",
    "# 输出序列起始标记\n",
    "GO_ID = 1\n",
    "# 结尾标记\n",
    "EOS_ID = 2\n",
    "# LSTM神经元size\n",
    "size = 8\n",
    "# 初始学习率\n",
    "init_learning_rate = 1\n",
    "# 在样本中出现频率超过这个值才会进入词表\n",
    "min_freq = 10\n",
    "\n",
    "# encoder:\n",
    "# 第一个样本\n",
    "encoder_input_0 = [PAD_ID] * (input_seq_len - len(train_set[0][0])) + train_set[0][0]\n",
    "\n",
    "# 第二个样本\n",
    "encoder_input_1 = [PAD_ID] * (input_seq_len - len(train_set[1][0])) + train_set[1][0]\n",
    "\n",
    "# decoder：\n",
    "decoder_input_0 = [GO_ID] + train_set[0][1] \n",
    "    + [PAD_ID] * (output_seq_len - len(train_set[0][1]) - 1)\n",
    "decoder_input_1 = [GO_ID] + train_set[1][1] \n",
    "    + [PAD_ID] * (output_seq_len - len(train_set[1][1]) - 1)\n",
    "    \n",
    "    \n",
    "# 格式转化\n",
    "encoder_inputs = []\n",
    "decoder_inputs = []\n",
    "for length_idx in xrange(input_seq_len):\n",
    "    encoder_inputs.append(np.array([encoder_input_0[length_idx], \n",
    "                          encoder_input_1[length_idx]], dtype=np.int32))\n",
    "for length_idx in xrange(output_seq_len):\n",
    "    decoder_inputs.append(np.array([decoder_input_0[length_idx], \n",
    "                          decoder_input_1[length_idx]], dtype=np.int32))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 打印出的encoder_inputs和decoder_inputs如下\n",
    "[\n",
    "  array([0, 0], dtype=int32), \n",
    "  array([0, 0], dtype=int32), \n",
    "  array([ 1, 23], dtype=int32), \n",
    "  array([ 3, 25], dtype=int32), \n",
    "  array([ 5, 27], dtype=int32)\n",
    "]\n",
    "[\n",
    "  array([1, 1], dtype=int32), \n",
    "  array([ 7, 29], dtype=int32), \n",
    "  array([ 9, 31], dtype=int32), \n",
    "  array([11, 33], dtype=int32), \n",
    "  array([0, 0], dtype=int32)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整体模块\n",
    "# coding:utf-8\n",
    "import numpy as np\n",
    "\n",
    "# 输入序列长度\n",
    "input_seq_len = 5\n",
    "# 输出序列长度\n",
    "output_seq_len = 5\n",
    "# 空值填充0\n",
    "PAD_ID = 0\n",
    "# 输出序列起始标记\n",
    "GO_ID = 1\n",
    "\n",
    "\n",
    "def get_samples():\n",
    "    \"\"\"构造样本数据\n",
    "\n",
    "    :return:\n",
    "        encoder_inputs: [array([0, 0], dtype=int32), \n",
    "                         array([0, 0], dtype=int32), \n",
    "                         array([1, 3], dtype=int32),\n",
    "                         array([3, 5], dtype=int32), \n",
    "                         array([5, 7], dtype=int32)]\n",
    "        decoder_inputs: [array([1, 1], dtype=int32), \n",
    "                         array([7, 9], dtype=int32), \n",
    "                         array([ 9, 11], dtype=int32),\n",
    "                         array([11, 13], dtype=int32), \n",
    "                         array([0, 0], dtype=int32)]\n",
    "    \"\"\"\n",
    "    train_set = [[[1, 3, 5], [7, 9, 11]], [[3, 5, 7], [9, 11, 13]]]\n",
    "    encoder_input_0 = [PAD_ID] * (input_seq_len - len(train_set[0][0])) \n",
    "                      + train_set[0][0]\n",
    "    encoder_input_1 = [PAD_ID] * (input_seq_len - len(train_set[1][0])) \n",
    "                      + train_set[1][0]\n",
    "    decoder_input_0 = [GO_ID] + train_set[0][1] \n",
    "                      + [PAD_ID] * (output_seq_len - len(train_set[0][1]) - 1)\n",
    "    decoder_input_1 = [GO_ID] + train_set[1][1] \n",
    "                      + [PAD_ID] * (output_seq_len - len(train_set[1][1]) - 1)\n",
    "\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    for length_idx in xrange(input_seq_len):\n",
    "        encoder_inputs.append(np.array([encoder_input_0[length_idx], \n",
    "                              encoder_input_1[length_idx]], dtype=np.int32))\n",
    "    for length_idx in xrange(output_seq_len):\n",
    "        decoder_inputs.append(np.array([decoder_input_0[length_idx], \n",
    "                              decoder_input_1[length_idx]], dtype=np.int32))\n",
    "    return encoder_inputs, decoder_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"构造模型\n",
    "    \"\"\"\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    for i in xrange(input_seq_len):\n",
    "        encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], \n",
    "                          name=\"encoder{0}\".format(i)))\n",
    "    for i in xrange(output_seq_len):\n",
    "        decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], \n",
    "                          name=\"decoder{0}\".format(i)))\n",
    "\n",
    "    # 创建一个记忆单元数目为size=8的LSTM神经元结构\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(size)\n",
    "\n",
    "    # 这里输出的状态我们不需要\n",
    "    outputs, _ = seq2seq.embedding_attention_seq2seq(\n",
    "                        encoder_inputs,\n",
    "                        decoder_inputs,\n",
    "                        cell,\n",
    "                        num_encoder_symbols=num_encoder_symbols,\n",
    "                        num_decoder_symbols=num_decoder_symbols,\n",
    "                        embedding_size=size,\n",
    "                        output_projection=None,\n",
    "                        feed_previous=False,\n",
    "                        dtype=tf.float32)\n",
    "    return encoder_inputs, decoder_inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
