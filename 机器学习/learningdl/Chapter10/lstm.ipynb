{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM的代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tokenFile\n",
    "import numpy as np\n",
    "\n",
    "# 输出单元激活函数\n",
    "def softmax(x):\n",
    "    x = np.array(x)\n",
    "    max_x = np.max(x)\n",
    "    return np.exp(x-max_x) / np.sum(np.exp(x-max_x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "# 定义lstm类\n",
    "class myLSTM:\n",
    "    def __init__(self, data_dim, hidden_dim=100):\n",
    "        # data_dim: 词向量维度，即词典长度; hidden_dim: 隐单元维度\n",
    "        self.data_dim = data_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # 初始化权重向量 \n",
    "        self.whi, self.wxi, self.bi = self._init_wh_wx()\n",
    "        self.whf, self.wxf, self.bf = self._init_wh_wx()                           \n",
    "        self.who, self.wxo, self.bo = self._init_wh_wx()\n",
    "        self.wha, self.wxa, self.ba = self._init_wh_wx()\n",
    "        self.wy, self.by = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
    "                                   (self.data_dim, self.hidden_dim)), \\\n",
    "                           np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
    "                                   (self.data_dim, 1))\n",
    "\n",
    "    # 初始化 wh, wx, b\n",
    "    def _init_wh_wx(self):\n",
    "        wh = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
    "                                   (self.hidden_dim, self.hidden_dim))\n",
    "        wx = np.random.uniform(-np.sqrt(1.0/self.data_dim), np.sqrt(1.0/self.data_dim), \n",
    "                                   (self.hidden_dim, self.data_dim))\n",
    "        b = np.random.uniform(-np.sqrt(1.0/self.data_dim), np.sqrt(1.0/self.data_dim), \n",
    "                                   (self.hidden_dim, 1))\n",
    "\n",
    "        return wh, wx, b\n",
    "\n",
    "    # 初始化各个状态向量\n",
    "    def _init_s(self, T):\n",
    "        iss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # input gate\n",
    "        fss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # forget gate\n",
    "        oss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # output gate\n",
    "        ass = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # current inputstate\n",
    "        hss = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # hidden state\n",
    "        css = np.array([np.zeros((self.hidden_dim, 1))] * (T + 1))  # cell state\n",
    "        ys = np.array([np.zeros((self.data_dim, 1))] * T)    # output value\n",
    "\n",
    "        return {'iss': iss, 'fss': fss, 'oss': oss, \n",
    "                'ass': ass, 'hss': hss, 'css': css, \n",
    "                'ys': ys}\n",
    "\n",
    "    # 前向传播，单个x\n",
    "    def forward(self, x):\n",
    "        # 向量时间长度\n",
    "        T = len(x)        \n",
    "        # 初始化各个状态向量\n",
    "        stats = self._init_s(T)               \n",
    "\n",
    "        for t in range(T):\n",
    "            # 前一时刻隐藏状态\n",
    "            ht_pre = np.array(stats['hss'][t-1]).reshape(-1, 1)\n",
    "\n",
    "            # input gate\n",
    "            stats['iss'][t] = self._cal_gate(self.whi, self.wxi, self.bi, ht_pre, x[t], sigmoid)\n",
    "            # forget gate\n",
    "            stats['fss'][t] = self._cal_gate(self.whf, self.wxf, self.bf, ht_pre, x[t], sigmoid)\n",
    "            # output gate\n",
    "            stats['oss'][t] = self._cal_gate(self.who, self.wxo, self.bo, ht_pre, x[t], sigmoid)\n",
    "            # current inputstate\n",
    "            stats['ass'][t] = self._cal_gate(self.wha, self.wxa, self.ba, ht_pre, x[t], tanh)\n",
    "\n",
    "            # cell state, ct = ft * ct_pre + it * at\n",
    "            stats['css'][t] = stats['fss'][t] * stats['css'][t-1] + stats['iss'][t] * stats['ass'][t]            \n",
    "            # hidden state, ht = ot * tanh(ct)\n",
    "            stats['hss'][t] = stats['oss'][t] * tanh(stats['css'][t])\n",
    "\n",
    "            # output value, yt = softmax(self.wy.dot(ht) + self.by)\n",
    "            stats['ys'][t] = softmax(self.wy.dot(stats['hss'][t]) + self.by)\n",
    "\n",
    "        return stats\n",
    "\n",
    "    # 计算各个门的输出\n",
    "    def _cal_gate(self, wh, wx, b, ht_pre, x, activation):\n",
    "        return activation(wh.dot(ht_pre) + wx[:, x].reshape(-1,1) + b)\n",
    "\n",
    "    # 预测输出，单个x    \n",
    "    def predict(self, x):\n",
    "        stats = self.forward(x)\n",
    "        pre_y = np.argmax(stats['ys'].reshape(len(x), -1), axis=1)         \n",
    "        return pre_y\n",
    "\n",
    "    # 计算损失， softmax交叉熵损失函数， (x,y)为多个样本\n",
    "    def loss(self, x, y):\n",
    "        cost = 0        \n",
    "        for i in xrange(len(y)):\n",
    "            stats = self.forward(x[i])\n",
    "            # 取出 y[i] 中每一时刻对应的预测值\n",
    "            pre_yi = stats['ys'][xrange(len(y[i])), y[i]]\n",
    "            cost -= np.sum(np.log(pre_yi))\n",
    "\n",
    "        # 统计所有y中词的个数, 计算平均损失\n",
    "        N = np.sum([len(yi) for yi in y])\n",
    "        ave_loss = cost / N\n",
    "\n",
    "        return ave_loss\n",
    "\n",
    "     # 初始化偏导数 dwh, dwx, db\n",
    "    def _init_wh_wx_grad(self):\n",
    "        dwh = np.zeros(self.whi.shape)\n",
    "        dwx = np.zeros(self.wxi.shape)\n",
    "        db = np.zeros(self.bi.shape)\n",
    "\n",
    "        return dwh, dwx, db\n",
    "\n",
    "    # 求梯度, (x,y)为一个样本\n",
    "    def bptt(self, x, y):\n",
    "        dwhi, dwxi, dbi = self._init_wh_wx_grad()\n",
    "        dwhf, dwxf, dbf = self._init_wh_wx_grad()                           \n",
    "        dwho, dwxo, dbo = self._init_wh_wx_grad()\n",
    "        dwha, dwxa, dba = self._init_wh_wx_grad()\n",
    "        dwy, dby = np.zeros(self.wy.shape), np.zeros(self.by.shape)\n",
    "\n",
    "        # 初始化 delta_ct，因为反向传播过程中，此值需要累加\n",
    "        delta_ct = np.zeros((self.hidden_dim, 1))\n",
    "\n",
    "        # 前向计算\n",
    "        stats = self.forward(x)\n",
    "        # 目标函数对输出 y 的偏导数\n",
    "        delta_o = stats['ys']\n",
    "        delta_o[np.arange(len(y)), y] -= 1\n",
    "\n",
    "        for t in np.arange(len(y))[::-1]:\n",
    "            # 输出层wy, by的偏导数，由于所有时刻的输出共享输出权值矩阵，故所有时刻累加\n",
    "            dwy += delta_o[t].dot(stats['hss'][t].reshape(1, -1))  \n",
    "            dby += delta_o[t]\n",
    "\n",
    "            # 目标函数对隐藏状态的偏导数\n",
    "            delta_ht = self.wy.T.dot(delta_o[t])\n",
    "\n",
    "            # 各个门及状态单元的偏导数\n",
    "            delta_ot = delta_ht * tanh(stats['css'][t])\n",
    "            delta_ct += delta_ht * stats['oss'][t] * (1-tanh(stats['css'][t])**2)\n",
    "            delta_it = delta_ct * stats['ass'][t]\n",
    "            delta_ft = delta_ct * stats['css'][t-1]\n",
    "            delta_at = delta_ct * stats['iss'][t]\n",
    "\n",
    "            delta_at_net = delta_at * (1-stats['ass'][t]**2)\n",
    "            delta_it_net = delta_it * stats['iss'][t] * (1-stats['iss'][t])\n",
    "            delta_ft_net = delta_ft * stats['fss'][t] * (1-stats['fss'][t])\n",
    "            delta_ot_net = delta_ot * stats['oss'][t] * (1-stats['oss'][t])\n",
    "\n",
    "            # 更新各权重矩阵的偏导数，由于所有时刻共享权值，故所有时刻累加\n",
    "            dwhf, dwxf, dbf = self._cal_grad_delta(dwhf, dwxf, dbf, delta_ft_net, stats['hss'][t-1], x[t])                              \n",
    "            dwhi, dwxi, dbi = self._cal_grad_delta(dwhi, dwxi, dbi, delta_it_net, stats['hss'][t-1], x[t])                              \n",
    "            dwha, dwxa, dba = self._cal_grad_delta(dwha, dwxa, dba, delta_at_net, stats['hss'][t-1], x[t])            \n",
    "            dwho, dwxo, dbo = self._cal_grad_delta(dwho, dwxo, dbo, delta_ot_net, stats['hss'][t-1], x[t])\n",
    "\n",
    "        return [dwhf, dwxf, dbf, \n",
    "                dwhi, dwxi, dbi, \n",
    "                dwha, dwxa, dba, \n",
    "                dwho, dwxo, dbo, \n",
    "                dwy, dby]\n",
    "\n",
    "    # 更新各权重矩阵的偏导数            \n",
    "    def _cal_grad_delta(self, dwh, dwx, db, delta_net, ht_pre, x):\n",
    "        dwh += delta_net * ht_pre\n",
    "        dwx += delta_net * x\n",
    "        db += delta_net\n",
    "\n",
    "        return dwh, dwx, db\n",
    "\n",
    "    # 计算梯度, (x,y)为一个样本\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dwhf, dwxf, dbf, \\\n",
    "        dwhi, dwxi, dbi, \\\n",
    "        dwha, dwxa, dba, \\\n",
    "        dwho, dwxo, dbo, \\\n",
    "        dwy, dby = self.bptt(x, y)\n",
    "\n",
    "        # 更新权重矩阵\n",
    "        self.whf, self.wxf, self.bf = self._update_wh_wx(learning_rate, self.whf, self.wxf, self.bf, dwhf, dwxf, dbf)\n",
    "        self.whi, self.wxi, self.bi = self._update_wh_wx(learning_rate, self.whi, self.wxi, self.bi, dwhi, dwxi, dbi)\n",
    "        self.wha, self.wxa, self.ba = self._update_wh_wx(learning_rate, self.wha, self.wxa, self.ba, dwha, dwxa, dba)\n",
    "        self.who, self.wxo, self.bo = self._update_wh_wx(learning_rate, self.who, self.wxo, self.bo, dwho, dwxo, dbo)\n",
    "\n",
    "        self.wy, self.by = self.wy - learning_rate * dwy, self.by - learning_rate * dby\n",
    "\n",
    "    # 更新权重矩阵\n",
    "    def _update_wh_wx(self, learning_rate, wh, wx, b, dwh, dwx, db):\n",
    "        wh -= learning_rate * dwh\n",
    "        wx -= learning_rate * dwx\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        return wh, wx, b\n",
    "\n",
    "    # 训练 LSTM\n",
    "    def train(self, X_train, y_train, learning_rate=0.005, n_epoch=5):\n",
    "        losses = []\n",
    "        num_examples = 0\n",
    "\n",
    "        for epoch in xrange(n_epoch):   \n",
    "            for i in xrange(len(y_train)):\n",
    "                self.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples += 1\n",
    "\n",
    "            loss = self.loss(X_train, y_train)\n",
    "            losses.append(loss)\n",
    "            print 'epoch {0}: loss = {1}'.format(epoch+1, loss)\n",
    "            if len(losses) > 1 and losses[-1] > losses[-2]:\n",
    "                learning_rate *= 0.5\n",
    "                print 'decrease learning_rate to', learning_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
