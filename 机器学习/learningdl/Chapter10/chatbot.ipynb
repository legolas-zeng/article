{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教你做一个简单的聊天机器人！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的工作原理\n",
    "三个模块：\n",
    "- 提问处理模块:\n",
    "    - 查询关键词生成、答案类型确定、句法和语义分析\n",
    "- 检索模块:\n",
    "    - 根据查询关键词所信息检索，返回句子或段落\n",
    "- 答案抽取模块:\n",
    "    - 通过分析和推理从检索出的句子或段落里抽取出和提问一致的实体，再根据概率最大对候选答案排序,最后选出最终的回答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的关键技术\n",
    "- 海量文本知识表示\n",
    "    - 网络文本资源获取、机器学习方法、大规模语义计算和推理、知识表示体系、知识库构建；\n",
    "\n",
    "- 问句解析\n",
    "    - 中文分词、词性标注、实体标注、概念类别标注、句法分析、语义分析、逻辑结构标注、指代消解、关联关系标注、问句分类（简单问句还是复杂问句、实体型还是段落型还是篇章级问题）、答案类别确定；\n",
    "\n",
    "- 答案生成与过滤\n",
    "    - 候选答案抽取、关系推演（并列关系还是递进关系还是因果关系）、吻合程度判断、噪声过滤\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的技术方法\n",
    "- 基于检索的技术\n",
    "- 基于模式匹配的技术\n",
    "- 基于自然语言理解的技术\n",
    "- 基于统计翻译模型的技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聊天机器人的常用模型\n",
    "- 循环神经网络和LSTM\n",
    "- seq2seq\n",
    "- attention模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow的seq2seq模型api\n",
    "embedding_attention_seq2seq(\n",
    "    encoder_inputs,\n",
    "    decoder_inputs,\n",
    "    cell,\n",
    "    num_encoder_symbols,\n",
    "    num_decoder_symbols,\n",
    "    embedding_size,\n",
    "    num_heads=1,\n",
    "    output_projection=None,\n",
    "    feed_previous=False,\n",
    "    dtype=None,\n",
    "    scope=None,\n",
    "    initial_state_attention=False\n",
    ") \n",
    "\n",
    "\n",
    "# 参数encoder_inputs是一个list，list中每一项是1D的Tensor，\n",
    "# 这个Tensor的shape是[batch_size]，Tensor中每一项是一个整数，类似这样：\n",
    "[array([0, 0, 0, 0], dtype=int32), \n",
    "array([0, 0, 0, 0], dtype=int32), \n",
    "array([8, 3, 5, 3], dtype=int32), \n",
    "array([7, 8, 2, 1], dtype=int32), \n",
    "array([6, 2, 10, 9], dtype=int32)]\n",
    "\n",
    "\n",
    "# 它的返回值是一个(outputs, state)结构的tuple，\n",
    "# 其中outputs是一个长度为句子长度(词数，与上面encoder_inputs的list长度一样)的list，\n",
    "# list中每一项是一个2D的tf.float32类型的Tensor，\n",
    "# 第一维度是样本数，比如4个样本则有四组Tensor，每个Tensor长度是embedding_size，像下面的样子：\n",
    "[array([\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143],\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143],\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143],\n",
    "      [-0.02027004, -0.017872  , -0.00233014, -0.0437047 ,  0.00083584,\n",
    "      0.01339234,  0.02355197,  0.02923143]\n",
    "    ],dtype=float32),\n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "    array([\n",
    "    ......\n",
    "    ],dtype=float32),  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0], dtype=int32), array([0, 0], dtype=int32), array([1, 3], dtype=int32), array([3, 5], dtype=int32), array([5, 7], dtype=int32)]\n",
      "[array([1, 1], dtype=int32), array([7, 9], dtype=int32), array([ 9, 11], dtype=int32), array([11, 13], dtype=int32), array([0, 0], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 输入序列长度\n",
    "input_seq_len = 5\n",
    "# 输出序列长度\n",
    "output_seq_len = 5\n",
    "# 空值填充0\n",
    "PAD_ID = 0\n",
    "# 输出序列起始标记\n",
    "GO_ID = 1\n",
    "# 结尾标记\n",
    "EOS_ID = 2\n",
    "# LSTM神经元size\n",
    "size = 8\n",
    "# 初始学习率\n",
    "init_learning_rate = 1\n",
    "# 在样本中出现频率超过这个值才会进入词表\n",
    "min_freq = 10\n",
    "\n",
    "a = [[1,3,5],[7,9,11]]\n",
    "b = [[3,5,7],[9,11,13]]\n",
    "\n",
    "train_set = [[[1,3,5],[7,9,11]],[[3,5,7],[9,11,13]]]\n",
    "\n",
    "# encoder:\n",
    "# 第一个样本\n",
    "encoder_input_0 = [PAD_ID] * (input_seq_len - len(train_set[0][0])) + train_set[0][0]\n",
    "\n",
    "# 第二个样本\n",
    "encoder_input_1 = [PAD_ID] * (input_seq_len - len(train_set[1][0])) + train_set[1][0]\n",
    "\n",
    "# decoder：\n",
    "decoder_input_0 = [GO_ID] + train_set[0][1] + [PAD_ID] * (output_seq_len - len(train_set[0][1]) - 1)\n",
    "decoder_input_1 = [GO_ID] + train_set[1][1] + [PAD_ID] * (output_seq_len - len(train_set[1][1]) - 1)\n",
    "    \n",
    "    \n",
    "# 格式转化\n",
    "encoder_inputs = []\n",
    "decoder_inputs = []\n",
    "for length_idx in xrange(input_seq_len):\n",
    "    encoder_inputs.append(np.array([encoder_input_0[length_idx], \n",
    "                          encoder_input_1[length_idx]], dtype=np.int32))\n",
    "for length_idx in xrange(output_seq_len):\n",
    "    decoder_inputs.append(np.array([decoder_input_0[length_idx], \n",
    "                          decoder_input_1[length_idx]], dtype=np.int32))\n",
    "print encoder_inputs\n",
    "print decoder_inputs\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 打印出的encoder_inputs和decoder_inputs如下\n",
    "[\n",
    "  array([0, 0], dtype=int32), \n",
    "  array([0, 0], dtype=int32), \n",
    "  array([ 1, 23], dtype=int32), \n",
    "  array([ 3, 25], dtype=int32), \n",
    "  array([ 5, 27], dtype=int32)\n",
    "]\n",
    "[\n",
    "  array([1, 1], dtype=int32), \n",
    "  array([ 7, 29], dtype=int32), \n",
    "  array([ 9, 31], dtype=int32), \n",
    "  array([11, 33], dtype=int32), \n",
    "  array([0, 0], dtype=int32)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整体模块\n",
    "# coding:utf-8\n",
    "import numpy as np\n",
    "\n",
    "# 输入序列长度\n",
    "input_seq_len = 5\n",
    "# 输出序列长度\n",
    "output_seq_len = 5\n",
    "# 空值填充0\n",
    "PAD_ID = 0\n",
    "# 输出序列起始标记\n",
    "GO_ID = 1\n",
    "\n",
    "\n",
    "def get_samples():\n",
    "    \"\"\"构造样本数据\n",
    "\n",
    "    :return:\n",
    "        encoder_inputs: [array([0, 0], dtype=int32), \n",
    "                         array([0, 0], dtype=int32), \n",
    "                         array([1, 3], dtype=int32),\n",
    "                         array([3, 5], dtype=int32), \n",
    "                         array([5, 7], dtype=int32)]\n",
    "        decoder_inputs: [array([1, 1], dtype=int32), \n",
    "                         array([7, 9], dtype=int32), \n",
    "                         array([ 9, 11], dtype=int32),\n",
    "                         array([11, 13], dtype=int32), \n",
    "                         array([0, 0], dtype=int32)]\n",
    "    \"\"\"\n",
    "    train_set = [[[1, 3, 5], [7, 9, 11]], [[3, 5, 7], [9, 11, 13]]]\n",
    "    encoder_input_0 = [PAD_ID] * (input_seq_len - len(train_set[0][0])) \n",
    "                      + train_set[0][0]\n",
    "    encoder_input_1 = [PAD_ID] * (input_seq_len - len(train_set[1][0])) \n",
    "                      + train_set[1][0]\n",
    "    decoder_input_0 = [GO_ID] + train_set[0][1] \n",
    "                      + [PAD_ID] * (output_seq_len - len(train_set[0][1]) - 1)\n",
    "    decoder_input_1 = [GO_ID] + train_set[1][1] \n",
    "                      + [PAD_ID] * (output_seq_len - len(train_set[1][1]) - 1)\n",
    "\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    for length_idx in xrange(input_seq_len):\n",
    "        encoder_inputs.append(np.array([encoder_input_0[length_idx], \n",
    "                              encoder_input_1[length_idx]], dtype=np.int32))\n",
    "    for length_idx in xrange(output_seq_len):\n",
    "        decoder_inputs.append(np.array([decoder_input_0[length_idx], \n",
    "                              decoder_input_1[length_idx]], dtype=np.int32))\n",
    "    return encoder_inputs, decoder_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"构造模型\n",
    "    \"\"\"\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    for i in xrange(input_seq_len):\n",
    "        encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], \n",
    "                          name=\"encoder{0}\".format(i)))\n",
    "    for i in xrange(output_seq_len):\n",
    "        decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], \n",
    "                          name=\"decoder{0}\".format(i)))\n",
    "\n",
    "    # 创建一个记忆单元数目为size=8的LSTM神经元结构\n",
    "    size = 8\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(size)\n",
    "\n",
    "    # 这里输出的状态我们不需要\n",
    "    outputs, _ = seq2seq.embedding_attention_seq2seq(\n",
    "                        encoder_inputs,\n",
    "                        decoder_inputs,\n",
    "                        cell,\n",
    "                        num_encoder_symbols=num_encoder_symbols,\n",
    "                        num_decoder_symbols=num_decoder_symbols,\n",
    "                        embedding_size=size,\n",
    "                        output_projection=None,\n",
    "                        feed_previous=False,\n",
    "                        dtype=tf.float32)\n",
    "    return encoder_inputs, decoder_inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sample_encoder_inputs, sample_decoder_inputs = get_samples()\n",
    "    encoder_inputs, decoder_inputs, outputs = get_model()\n",
    "    input_feed = {}\n",
    "    for l in xrange(input_seq_len):\n",
    "        input_feed[encoder_inputs[l].name] = sample_encoder_inputs[l]\n",
    "    for l in xrange(output_seq_len):\n",
    "        input_feed[decoder_inputs[l].name] = sample_decoder_inputs[l]\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    outputs = sess.run(outputs, input_feed)\n",
    "    print outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    训练过程\n",
    "    \"\"\"\n",
    "    # train_set = [[[5, 7, 9], [11, 13, 15, EOS_ID]], [[7, 9, 11], [13, 15, 17, EOS_ID]],\n",
    "    #              [[15, 17, 19], [21, 23, 25, EOS_ID]]]\n",
    "    train_set = get_train_set()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        encoder_inputs, decoder_inputs, target_weights, outputs, loss, update, saver, learning_rate_decay_op, learning_rate = get_model()\n",
    "\n",
    "        # 全部变量初始化\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # 训练很多次迭代，每隔10次打印一次loss，可以看情况直接ctrl+c停止\n",
    "        previous_losses = []\n",
    "        for step in xrange(20000):\n",
    "            sample_encoder_inputs, sample_decoder_inputs, sample_target_weights = get_samples(train_set, 1000)\n",
    "            input_feed = {}\n",
    "            for l in xrange(input_seq_len):\n",
    "                input_feed[encoder_inputs[l].name] = sample_encoder_inputs[l]\n",
    "            for l in xrange(output_seq_len):\n",
    "                input_feed[decoder_inputs[l].name] = sample_decoder_inputs[l]\n",
    "                input_feed[target_weights[l].name] = sample_target_weights[l]\n",
    "            input_feed[decoder_inputs[output_seq_len].name] = np.zeros([len(sample_decoder_inputs[0])], dtype=np.int32)\n",
    "            [loss_ret, _] = sess.run([loss, update], input_feed)\n",
    "            if step % 10 == 0:\n",
    "                print 'step=', step, 'loss=', loss_ret, 'learning_rate=', learning_rate.eval()\n",
    "\n",
    "                if len(previous_losses) > 5 and loss_ret > max(previous_losses[-5:]):\n",
    "                    sess.run(learning_rate_decay_op)\n",
    "                previous_losses.append(loss_ret)\n",
    "\n",
    "                # 模型持久化\n",
    "                saver.save(sess, './model/demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    \"\"\"\n",
    "    预测过程\n",
    "    \"\"\"\n",
    "    with tf.Session() as sess:\n",
    "        encoder_inputs, decoder_inputs, target_weights, outputs, loss, update, saver, learning_rate_decay_op, learning_rate = get_model(feed_previous=True)\n",
    "        saver.restore(sess, './model/demo')\n",
    "        sys.stdout.write(\"> \")\n",
    "        sys.stdout.flush()\n",
    "        input_seq = sys.stdin.readline()\n",
    "        while input_seq:\n",
    "            input_seq = input_seq.strip()\n",
    "            input_id_list = get_id_list_from(input_seq)\n",
    "            if (len(input_id_list)):\n",
    "                sample_encoder_inputs, sample_decoder_inputs, sample_target_weights = seq_to_encoder(' '.join([str(v) for v in input_id_list]))\n",
    "\n",
    "                input_feed = {}\n",
    "                for l in xrange(input_seq_len):\n",
    "                    input_feed[encoder_inputs[l].name] = sample_encoder_inputs[l]\n",
    "                for l in xrange(output_seq_len):\n",
    "                    input_feed[decoder_inputs[l].name] = sample_decoder_inputs[l]\n",
    "                    input_feed[target_weights[l].name] = sample_target_weights[l]\n",
    "                input_feed[decoder_inputs[output_seq_len].name] = np.zeros([2], dtype=np.int32)\n",
    "\n",
    "                # 预测输出\n",
    "                outputs_seq = sess.run(outputs, input_feed)\n",
    "                # 因为输出数据每一个是num_decoder_symbols维的，因此找到数值最大的那个就是预测的id，就是这里的argmax函数的功能\n",
    "                outputs_seq = [int(np.argmax(logit[0], axis=0)) for logit in outputs_seq]\n",
    "                # 如果是结尾符，那么后面的语句就不输出了\n",
    "                if EOS_ID in outputs_seq:\n",
    "                    outputs_seq = outputs_seq[:outputs_seq.index(EOS_ID)]\n",
    "                outputs_seq = [wordToken.id2word(v) for v in outputs_seq]\n",
    "                print \" \".join(outputs_seq)\n",
    "            else:\n",
    "                print \"WARN：词汇不在服务区\"\n",
    "\n",
    "            sys.stdout.write(\"> \")\n",
    "            sys.stdout.flush()\n",
    "            input_seq = sys.stdin.readline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
