{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用LSTM实现情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 了解数据集\n",
    "- pos.xls ：今天天气真好，心情也变得很好呢！\n",
    "- neg.xls ：下雨天把鞋弄湿了，好烦啊 ><!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "- 基本步骤\n",
    "\n",
    "    - 转换源数据编码格式为utf-8格式\n",
    "    - 过滤字符\n",
    "        - 去除所有非中文字符，如标点符号、英文字符、数字、网站链接等特殊字符。\n",
    "    - 过滤停用词\n",
    "    - 对文本内容进行分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#Created by huxiaoman 2018.1.28\n",
    "#transfer.py:生成pos和neg数据\n",
    "import jieba\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 判断邮件中的字符是否是中文\n",
    "def check_contain_chinese(check_str):\n",
    "    for ch in check_str.decode('utf-8'):\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 加载文本数据的label\n",
    "def load_label_files(label_file):\n",
    "    label_dict ={}\n",
    "    for line in open(label_file).readlines():\n",
    "        list1 = line.strip().split(\"..\")\n",
    "        label_dict[list1[1].strip()] = list1[0].strip()\n",
    "    return label_dict\n",
    "\n",
    "# 加载停用词词表\n",
    "def load_stop_train(stop_word_path):\n",
    "    stop_dict = {}\n",
    "    for line in open(stop_word_path).readlines():\n",
    "        line = line.strip()\n",
    "        stop_dict[line] = 1\n",
    "    return stop_dict\n",
    "\n",
    "# 读取文本数据，并转换为utf-8格式，生成pos和neg样本\n",
    "def read_files(file_path,label_dict,stop_dict,pos_file_path,neg_file_path):\n",
    "    parents = os.listdir(file_path)\n",
    "    pos_file = open(pos_file_path,'a')\n",
    "    neg_file = open(neg_file_path,'a')\n",
    "    for parent in parents:\n",
    "        child = os.path.join(file_path,parent)\n",
    "        if os.path.isdir(child):\n",
    "            read_files(child,label_dict,stop_dict,pos_file_path,neg_file_path)\n",
    "        else:\n",
    "            print child[10:]\n",
    "            label = \"unk\"\n",
    "            if child[10:] in label_dict:\n",
    "                label = label_dict[child[10:]]\n",
    "            # deal file\n",
    "            temp_list = []\n",
    "            for line in open(child).readlines():\n",
    "                line = line.strip().decode(\"gbk\",'ignore').encode('utf-8')\n",
    "                if not check_contain_chinese(line):\n",
    "                    continue\n",
    "                seg_list = jieba.cut(line, cut_all=False)\n",
    "                for word in seg_list:\n",
    "                    if word in stop_dict:\n",
    "                        continue\n",
    "                    else:\n",
    "                        temp_list.append(word)\n",
    "            line = \" \".join(temp_list)\n",
    "            print label\n",
    "            if label == \"pos\":\n",
    "                pos_file.write(line.encode(\"utf-8\",\"ignore\") + \"\\n\")\n",
    "            if label == \"neg\":\n",
    "                neg_file.write(line.encode(\"utf-8\",\"ignore\")+\"\\n\")\n",
    "\n",
    "# 生成word2vec词表\n",
    "def generate_word2vec(file_path,label_dict,stop_dict,word_vec):\n",
    "    parents = os.listdir(file_path)\n",
    "    fh1 = open(word_vec,'a')\n",
    "    i = 0\n",
    "\n",
    "    for parent in parents:\n",
    "        child = os.path.join(file_path,parent)\n",
    "        if os.path.isdir(child):\n",
    "            generate_word2vec(child,label_dict,stop_dict,word_vec)\n",
    "        else:\n",
    "            print child[10:]\n",
    "            i += 1\n",
    "            print i\n",
    "            label = \"unk\"\n",
    "            if child[10:] in label_dict:\n",
    "                label = label_dict[child[10:]]\n",
    "            # deal file\n",
    "            temp_list = []\n",
    "            for line in open(child).readlines():\n",
    "                line = line.strip().decode(\"gbk\",'ignore').encode('utf-8')\n",
    "                if not check_contain_chinese(line):\n",
    "                    continue\n",
    "                if len(line) == 0:\n",
    "                    continue\n",
    "                seg_list = jieba.cut(line, cut_all=False)\n",
    "                for word in seg_list:\n",
    "                    if word in stop_dict:\n",
    "                        continue\n",
    "                    else:\n",
    "                        temp_list.append(word)\n",
    "            line = \" \".join(temp_list)\n",
    "            fh1.write(line.encode(\"utf-8\",\"ingore\")+\"\\n\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    file_path = sys.argv[1]\n",
    "    label_path = sys.argv[2]\n",
    "    stop_word_path = \"stop_words.txt\"\n",
    "    word_vec_path = \"word2vec.txt\"\n",
    "    pos_data = \"pos.txt\"\n",
    "    neg_data = \"neg.txt\"\n",
    "    label_dict = load_label_files(label_path)\n",
    "    stop_dict = load_stop_train(stop_word_path)\n",
    "    read_files(file_path,label_dict,stop_dict,pos_data,neg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 运行方式 :run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bashif [ $1 = \"test\" ]; then\n",
    "    echo \"test\"\n",
    "    python transfer.py ../test/ ../data/pos.txt\n",
    "else\n",
    "    echo \"whole\"\n",
    "    python transfer.py ../data/ ../trec06c/full/index\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 运行方式\n",
    "sh run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 运行结果\n",
    "    - pos.txt: 正样本：正面情感\n",
    "        - 今天 天气 真好 心情 也 变得 很好 呢  \n",
    "    - neg.txt: 负样本：负面情感\n",
    "        - 下雨 天 把 鞋 弄湿 了 好烦 啊\n",
    "    - word2vec.txt: 所有文本的分词内容，为训练WordVec模型提供语料\n",
    "        - 今天 天气 真好 心情 也 变得 很好 呢 下雨 天 把 鞋 弄湿 了 好烦 啊\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成Word2vec模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Created by huxiaoman 2018.1.28\n",
    "# word2vec.py:生成word2vec模型\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import codecs\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding( \"utf-8\" )\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in codecs.open(os.path.join(self.dirname, fname),\"r\", encoding=\"utf-8\",errors=\"ignore\"):\n",
    "                yield line.strip().split()\n",
    "\n",
    "# word2vec.txt数据的地址\n",
    "train_path = \"rawData/\"\n",
    "\n",
    "# 生成的word2vec模型的地址\n",
    "model_path = \"/modelPath/\"\n",
    "sentences = MySentences(train_path) \n",
    "\n",
    "# 此处min_count=5代表5元模型，size=100代表词向量维度，worker=15表示15个线程\n",
    "model = Word2Vec(sentences,min_count = 5,size=100,workers=15)\n",
    "\n",
    "#保存模型\n",
    "model.save(model_path+'/Word2vec_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 运行方式： python word2vec.py\n",
    "- 运行结果： Word2vec_model.pkl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##定义网络结构\n",
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    nb_classes = 3\n",
    "    print 'Defining a Simple Keras Model...'\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    print vocab_dim\n",
    "    print n_symbols\n",
    "    #model.add(LSTM(output_dim=50, activation='relu',inner_activation='hard_sigmoid'))\n",
    "    #model.add(LSTM(output_dim=25, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(64, input_dim=vocab_dim, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(nb_classes))\n",
    "    #model.add(Activation('softmax'))\n",
    "    print model.summary()\n",
    "    model.add(NonMasking())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(output_dim=nb_classes, activation='softmax'))\n",
    "    print 'Compiling the Model...'\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print \"Train...\"\n",
    "    print y_train\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=n_epoch,verbose=1, validation_data=(x_test, y_test))\n",
    "    print \"Evaluate...\"\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('lstm_data/lstm_koubei.yml', 'w') as outfile:\n",
    "        outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "    model.save_weights('lstm_data/lstm_koubei.h5')\n",
    "    print 'Test score:', score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练模型，并保存\n",
    "def train():\n",
    "    print 'Loading Data...'\n",
    "    combined,y=loadfile()\n",
    "    print len(combined),len(y)\n",
    "    print 'Tokenising...'\n",
    "    combined = tokenizer(combined)\n",
    "    print 'Training a Word2vec model...'\n",
    "    index_dict, word_vectors,combined=word2vec_train(combined)\n",
    "    print 'Setting up Arrays for Keras Embedding Layer...'\n",
    "    n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined,y)\n",
    "    print x_train.shape,y_train.shape\n",
    "    train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
