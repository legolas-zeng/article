{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN的代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tokenFile\n",
    "import numpy as np\n",
    "\n",
    "# 输出单元激活函数\n",
    "def softmax(x):\n",
    "    x = np.array(x)\n",
    "    max_x = np.max(x)\n",
    "    return np.exp(x-max_x) / np.sum(np.exp(x-max_x))\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, data_dim, hidden_dim=100, bptt_back=4):\n",
    "        # data_dim: 词向量维度，即词典长度; hidden_dim: 隐单元维度; bptt_back: 反向传播回传时间长度\n",
    "        self.data_dim = data_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_back = bptt_back\n",
    "\n",
    "        # 初始化权重向量 U， W， V; U为输入权重; W为递归权重; V为输出权重\n",
    "        self.U = np.random.uniform(-np.sqrt(1.0/self.data_dim), np.sqrt(1.0/self.data_dim), \n",
    "                                   (self.hidden_dim, self.data_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
    "                                   (self.hidden_dim, self.hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1.0/self.hidden_dim), np.sqrt(1.0/self.hidden_dim), \n",
    "                                   (self.data_dim, self.hidden_dim))\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        # 向量时间长度\n",
    "        T = len(x)\n",
    "\n",
    "        # 初始化状态向量, s包含额外的初始状态 s[-1]\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        o = np.zeros((T, self.data_dim))\n",
    "\n",
    "        for t in xrange(T):\n",
    "            s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "\n",
    "        return [o, s]\n",
    "\n",
    "    # 预测输出        \n",
    "    def predict(self, x):\n",
    "        o, s = self.forward(x)\n",
    "        pre_y = np.argmax(o, axis=1)\n",
    "        return pre_y\n",
    "\n",
    "    # 计算损失， softmax损失函数， (x,y)为多个样本\n",
    "    def loss(self, x, y):\n",
    "        cost = 0        \n",
    "        for i in xrange(len(y)):\n",
    "            o, s = self.forward(x[i])\n",
    "            # 取出 y[i] 中每一时刻对应的预测值\n",
    "            pre_yi = o[xrange(len(y[i])), y[i]]\n",
    "            cost -= np.sum(np.log(pre_yi))\n",
    "\n",
    "        # 统计所有y中词的个数, 计算平均损失\n",
    "        N = np.sum([len(yi) for yi in y])\n",
    "        ave_loss = cost / N\n",
    "\n",
    "        return ave_loss\n",
    "\n",
    "    # 求梯度, (x,y)为一个样本\n",
    "    def bptt(self, x, y):\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "\n",
    "        o, s = self.forward(x)\n",
    "        delta_o = o\n",
    "        delta_o[xrange(len(y)), y] -= 1\n",
    "\n",
    "        for t in np.arange(len(y))[::-1]:\n",
    "            # 梯度沿输出层向输入层的传播\n",
    "            dV += delta_o[t].reshape(-1, 1) * s[t].reshape(1, -1)  # self.data_dim * self.hidden_dim\n",
    "            delta_t = delta_o[t].reshape(1, -1).dot(self.V) * ((1 - s[t-1]**2).reshape(1, -1)) # 1 * self.hidden_dim\n",
    "\n",
    "            # 梯度沿时间t的传播\n",
    "            for bpt_t in np.arange(np.max([0, t-self.bptt_back]), t+1)[::-1]:\n",
    "                dW += delta_t.T.dot(s[bpt_t-1].reshape(1, -1))\n",
    "                dU[:, x[bpt_t]] = dU[:, x[bpt_t]] + delta_t\n",
    "\n",
    "                delta_t = delta_t.dot(self.W.T) * (1 - s[bpt_t-1]**2)\n",
    "\n",
    "        return [dU, dW, dV]\n",
    "\n",
    "    # 更新权重  \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dU, dW, dV = self.bptt(x, y)\n",
    "\n",
    "        self.U -= learning_rate * dU\n",
    "        self.W -= learning_rate * dW\n",
    "        self.V -= learning_rate * dV\n",
    "\n",
    "    # 训练RNN  \n",
    "    def train(self, X_train, y_train, learning_rate=0.005, n_epoch=5):\n",
    "        losses = []\n",
    "        num_examples = 0\n",
    "\n",
    "        for epoch in xrange(n_epoch):   \n",
    "            for i in xrange(len(y_train)):\n",
    "                self.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "                num_examples += 1\n",
    "\n",
    "            loss = self.loss(X_train, y_train)\n",
    "            losses.append(loss)\n",
    "            print 'epoch {0}: loss = {1}'.format(epoch+1, loss)\n",
    "            # 若损失增加，降低学习率\n",
    "            if len(losses) > 1 and losses[-1] > losses[-2]:\n",
    "                learning_rate *= 0.5\n",
    "                print 'decrease learning_rate to', learning_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
