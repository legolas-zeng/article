{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 经典CNN网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le-Net5 网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input_tensor,train,regularizer):\n",
    "\n",
    "    #第一层：卷积层，过滤器的尺寸为5×5，深度为6,不使用全0补充，步长为1。\n",
    "    #尺寸变化：32×32×1->28×28×6\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable('weight',[5,5,1,6],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biases = tf.get_variable('bias',[6],initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor,conv1_weights,strides=[1,1,1,1],padding='VALID')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1,conv1_biases))\n",
    "\n",
    "    #第二层：池化层，过滤器的尺寸为2×2，使用全0补充，步长为2。\n",
    "    #尺寸变化：28×28×6->14×14×6\n",
    "    with tf.name_scope('layer2-pool1'):\n",
    "        pool1 = tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "    #第三层：卷积层，过滤器的尺寸为5×5，深度为16,不使用全0补充，步长为1。\n",
    "    #尺寸变化：14×14×6->10×10×16\n",
    "    with tf.variable_scope('layer3-conv2'):\n",
    "        conv2_weights = tf.get_variable('weight',[5,5,6,16],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable('bias',[16],initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1,conv2_weights,strides=[1,1,1,1],padding='VALID')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2,conv2_biases))\n",
    "\n",
    "    #第四层：池化层，过滤器的尺寸为2×2，使用全0补充，步长为2。\n",
    "    #尺寸变化：10×10×16->5×5×16\n",
    "    with tf.variable_scope('layer4-pool2'):\n",
    "        pool2 = tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "    #将第四层池化层的输出转化为第五层全连接层的输入格式。第四层的输出为5×5×16的矩阵，然而第五层全连接层需要的输入格式\n",
    "    #为向量，所以我们需要把代表每张图片的尺寸为5×5×16的矩阵拉直成一个长度为5×5×16的向量。\n",
    "    #举例说，每次训练64张图片，那么第四层池化层的输出的size为(64,5,5,16),拉直为向量，nodes=5×5×16=400,尺寸size变为(64,400)\n",
    "    pool_shape = pool2.get_shape().as_list()\n",
    "    nodes = pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped = tf.reshape(pool2,[-1,nodes])\n",
    "\n",
    "    #第五层：全连接层，nodes=5×5×16=400，400->120的全连接\n",
    "    #尺寸变化：比如一组训练样本为64，那么尺寸变化为64×400->64×120\n",
    "    #训练时，引入dropout，dropout在训练时会随机将部分节点的输出改为0，dropout可以避免过拟合问题。\n",
    "    #这和模型越简单越不容易过拟合思想一致，和正则化限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声，以此达到避免过拟合思想一致。\n",
    "    #本文最后训练时没有采用dropout，dropout项传入参数设置成了False，因为训练和测试写在了一起没有分离，不过大家可以尝试。\n",
    "    with tf.variable_scope('layer5-fc1'):\n",
    "        fc1_weights = tf.get_variable('weight',[nodes,120],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses',regularizer(fc1_weights))\n",
    "        fc1_biases = tf.get_variable('bias',[120],initializer=tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped,fc1_weights) + fc1_biases)\n",
    "        if train:\n",
    "            fc1 = tf.nn.dropout(fc1,0.5)\n",
    "\n",
    "    #第六层：全连接层，120->84的全连接\n",
    "    #尺寸变化：比如一组训练样本为64，那么尺寸变化为64×120->64×84\n",
    "    with tf.variable_scope('layer6-fc2'):\n",
    "        fc2_weights = tf.get_variable('weight',[120,84],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses',regularizer(fc2_weights))\n",
    "        fc2_biases = tf.get_variable('bias',[84],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1,fc2_weights) + fc2_biases)\n",
    "        if train:\n",
    "            fc2 = tf.nn.dropout(fc2,0.5)\n",
    "\n",
    "    #第七层：全连接层（近似表示），84->10的全连接\n",
    "    #尺寸变化：比如一组训练样本为64，那么尺寸变化为64×84->64×10。最后，64×10的矩阵经过softmax之后就得出了64张图片分类于每种数字的概率，\n",
    "    #即得到最后的分类结果。\n",
    "    with tf.variable_scope('layer7-fc3'):\n",
    "        fc3_weights = tf.get_variable('weight',[84,10],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        if regularizer != None:\n",
    "            tf.add_to_collection('losses',regularizer(fc3_weights))\n",
    "        fc3_biases = tf.get_variable('bias',[10],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        logit = tf.matmul(fc2,fc3_weights) + fc3_biases\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  \"\"\"\n",
    "  构建一个AlexNet模型\n",
    "\n",
    "  \"\"\"\n",
    "  parameters = []\n",
    "  # 第一层：卷积层conv1\n",
    "  with tf.name_scope('conv1') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([11, 11, 3, 96], dtype=tf.float32,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 4, 4, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[96], dtype=tf.float32),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(bias, name=scope)\n",
    "    print_activations(conv1)\n",
    "    parameters += [kernel, biases]\n",
    "\n",
    "\n",
    "  # 第二层：池化层pool1\n",
    "    pool1 = tf.nn.max_pool(conv1,\n",
    "                         ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='VALID',\n",
    "                         name='pool1')\n",
    "    print_activations(pool1)\n",
    "\n",
    "  # 第三层：卷积层2 conv2\n",
    "  with tf.name_scope('conv2') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([5, 5, 96, 256], dtype=tf.float32,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "    print_activations(conv2)\n",
    "\n",
    "  # 第四层：池化层2 pool2\n",
    "    pool2 = tf.nn.max_pool(conv2,\n",
    "                         ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='VALID',\n",
    "                         name='pool2')\n",
    "    print_activations(pool2)\n",
    "\n",
    "  # 第五层：卷积层3 conv3\n",
    "  with tf.name_scope('conv3') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 384],\n",
    "                                             dtype=tf.float32,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv3 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "    print_activations(conv3)\n",
    "\n",
    "  # 第六层：卷积层4 conv4\n",
    "  with tf.name_scope('conv4') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 384],\n",
    "                                             dtype=tf.float32,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv4 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "    print_activations(conv4)\n",
    "\n",
    "  # 第七层：卷积层5 conv5\n",
    "  with tf.name_scope('conv5') as scope:\n",
    "    kernel = tf.Variable(tf.truncated_normal([3, 3, 384, 256],\n",
    "                                             dtype=tf.float32,\n",
    "                                             stddev=1e-1), name='weights')\n",
    "    conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                         trainable=True, name='biases')\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv5 = tf.nn.relu(bias, name=scope)\n",
    "    parameters += [kernel, biases]\n",
    "    print_activations(conv5)\n",
    "\n",
    "  # 第八层：池化层 pool5\n",
    "    pool5 = tf.nn.max_pool(conv5,\n",
    "                         ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1],\n",
    "                         padding='VALID',\n",
    "                         name='pool5')\n",
    "    print_activations(pool5)\n",
    "\n",
    "    return pool5, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize\n",
    "from imagenet_classes import class_names\n",
    "\n",
    "\n",
    "class vgg16:\n",
    "    def __init__(self, imgs, weights=None, sess=None):\n",
    "        self.imgs = imgs\n",
    "        self.convlayers()\n",
    "        self.fc_layers()\n",
    "        self.probs = tf.nn.softmax(self.fc3l)\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "\n",
    "\n",
    "    def convlayers(self):\n",
    "        self.parameters = []\n",
    "\n",
    "        # zero-mean input\n",
    "        with tf.name_scope('preprocess') as scope:\n",
    "            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')\n",
    "            images = self.imgs-mean\n",
    "\n",
    "        # conv1_1\n",
    "        with tf.name_scope('conv1_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv1_2\n",
    "        with tf.name_scope('conv1_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv1_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool1\n",
    "        self.pool1 = tf.nn.max_pool(self.conv1_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool1')\n",
    "\n",
    "        # conv2_1\n",
    "        with tf.name_scope('conv2_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv2_2\n",
    "        with tf.name_scope('conv2_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv2_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool2\n",
    "        self.pool2 = tf.nn.max_pool(self.conv2_2,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool2')\n",
    "\n",
    "        # conv3_1\n",
    "        with tf.name_scope('conv3_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_2\n",
    "        with tf.name_scope('conv3_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv3_3\n",
    "        with tf.name_scope('conv3_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv3_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool3\n",
    "        self.pool3 = tf.nn.max_pool(self.conv3_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool3')\n",
    "\n",
    "        # conv4_1\n",
    "        with tf.name_scope('conv4_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_2\n",
    "        with tf.name_scope('conv4_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv4_3\n",
    "        with tf.name_scope('conv4_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv4_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool4\n",
    "        self.pool4 = tf.nn.max_pool(self.conv4_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "        # conv5_1\n",
    "        with tf.name_scope('conv5_1') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_1 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_2\n",
    "        with tf.name_scope('conv5_2') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_2 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # conv5_3\n",
    "        with tf.name_scope('conv5_3') as scope:\n",
    "            kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "            conv = tf.nn.conv2d(self.conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "            biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            out = tf.nn.bias_add(conv, biases)\n",
    "            self.conv5_3 = tf.nn.relu(out, name=scope)\n",
    "            self.parameters += [kernel, biases]\n",
    "\n",
    "        # pool5\n",
    "        self.pool5 = tf.nn.max_pool(self.conv5_3,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME',\n",
    "                               name='pool4')\n",
    "\n",
    "    def fc_layers(self):\n",
    "        # fc1\n",
    "        with tf.name_scope('fc1') as scope:\n",
    "            shape = int(np.prod(self.pool5.get_shape()[1:]))\n",
    "            fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            pool5_flat = tf.reshape(self.pool5, [-1, shape])\n",
    "            fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "            self.fc1 = tf.nn.relu(fc1l)\n",
    "            self.parameters += [fc1w, fc1b]\n",
    "\n",
    "        # fc2\n",
    "        with tf.name_scope('fc2') as scope:\n",
    "            fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            fc2l = tf.nn.bias_add(tf.matmul(self.fc1, fc2w), fc2b)\n",
    "            self.fc2 = tf.nn.relu(fc2l)\n",
    "            self.parameters += [fc2w, fc2b]\n",
    "\n",
    "        # fc3\n",
    "        with tf.name_scope('fc3') as scope:\n",
    "            fc3w = tf.Variable(tf.truncated_normal([4096, 1000],\n",
    "                                                         dtype=tf.float32,\n",
    "                                                         stddev=1e-1), name='weights')\n",
    "            fc3b = tf.Variable(tf.constant(1.0, shape=[1000], dtype=tf.float32),\n",
    "                                 trainable=True, name='biases')\n",
    "            self.fc3l = tf.nn.bias_add(tf.matmul(self.fc2, fc3w), fc3b)\n",
    "            self.parameters += [fc3w, fc3b]\n",
    "\n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            print i, k, np.shape(weights[k])\n",
    "            sess.run(self.parameters[i].assign(weights[k]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sess = tf.Session()\n",
    "    imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    vgg = vgg16(imgs, 'vgg16_weights.npz', sess)\n",
    "\n",
    "    img1 = imread('laska.png', mode='RGB')\n",
    "    img1 = imresize(img1, (224, 224))\n",
    "\n",
    "    prob = sess.run(vgg.probs, feed_dict={vgg.imgs: [img1]})[0]\n",
    "    preds = (np.argsort(prob)[::-1])[0:5]\n",
    "    for p in preds:\n",
    "        print class_names[p], prob[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########定义函数生成网络中经常用到的函数的默认参数########\n",
    "# 默认参数：卷积的激活函数、权重初始化方式、标准化器等\n",
    "def inception_v3_arg_scope(weight_decay=0.00004,  # 设置L2正则的weight_decay\n",
    "                           stddev=0.1, # 标准差默认值0.1\n",
    "                           batch_norm_var_collection='moving_vars'):\n",
    "\n",
    "    batch_norm_params = {  # 定义batch normalization（标准化）的参数字典\n",
    "      'decay': 0.9997,  # 定义参数衰减系数\n",
    "      'epsilon': 0.001,  \n",
    "      'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "      'variables_collections': {\n",
    "          'beta': None,\n",
    "          'gamma': None,\n",
    "          'moving_mean': [batch_norm_var_collection],\n",
    "          'moving_variance': [batch_norm_var_collection],\n",
    "      }\n",
    "  }\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected], # 给函数的参数自动赋予某些默认值\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay)): # 对[slim.conv2d, slim.fully_connected]自动赋值\n",
    "  # 使用slim.arg_scope后就不需要每次都重复设置参数了，只需要在有修改时设置\n",
    "        with slim.arg_scope( # 嵌套一个slim.arg_scope对卷积层生成函数slim.conv2d的几个参数赋予默认值\n",
    "            [slim.conv2d],\n",
    "            weights_initializer=trunc_normal(stddev), # 权重初始化器\n",
    "            activation_fn=tf.nn.relu, # 激活函数\n",
    "            normalizer_fn=slim.batch_norm, # 标准化器\n",
    "            normalizer_params=batch_norm_params) as sc: # 标准化器的参数设置为前面定义的batch_norm_params\n",
    "        return sc # 最后返回定义好的scope\n",
    "\n",
    "\n",
    "########定义函数可以生成Inception V3网络的卷积部分########\n",
    "def inception_v3_base(inputs, scope=None):\n",
    "  '''\n",
    "  Args:\n",
    "  inputs：输入的tensor\n",
    "  scope：包含了函数默认参数的环境\n",
    "  '''\n",
    "  end_points = {} # 定义一个字典表保存某些关键节点供之后使用\n",
    "\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs]):\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], # 对三个参数设置默认值\n",
    "                            stride=1, padding='VALID'):\n",
    "          # 正式定义Inception V3的网络结构。首先是前面的非Inception Module的卷积层\n",
    "          # 299 x 299 x 3\n",
    "          # 第一个参数为输入的tensor，第二个是输出的通道数，卷积核尺寸，步长stride，padding模式\n",
    "              net = slim.conv2d(inputs, 32, [3, 3], stride=2, scope='Conv2d_1a_3x3') # 直接使用slim.conv2d创建卷积层\n",
    "              # 149 x 149 x 32\n",
    "              '''\n",
    "              因为使用了slim以及slim.arg_scope，我们一行代码就可以定义好一个卷积层\n",
    "              相比AlexNet使用好几行代码定义一个卷积层，或是VGGNet中专门写一个函数定义卷积层，都更加方便\n",
    "              '''\n",
    "              net = slim.conv2d(net, 32, [3, 3], scope='Conv2d_2a_3x3')\n",
    "              # 147 x 147 x 32\n",
    "              net = slim.conv2d(net, 64, [3, 3], padding='SAME', scope='Conv2d_2b_3x3')\n",
    "              # 147 x 147 x 64\n",
    "              net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPool_3a_3x3')\n",
    "              # 73 x 73 x 64\n",
    "              net = slim.conv2d(net, 80, [1, 1], scope='Conv2d_3b_1x1')\n",
    "              # 73 x 73 x 80.\n",
    "              net = slim.conv2d(net, 192, [3, 3], scope='Conv2d_4a_3x3')\n",
    "              # 71 x 71 x 192.\n",
    "              net = slim.max_pool2d(net, [3, 3], stride=2, scope='MaxPool_5a_3x3')\n",
    "          # 35 x 35 x 192.\n",
    "\n",
    "          # 上面部分代码一共有5个卷积层，2个池化层，实现了对图片数据的尺寸压缩，并对图片特征进行了抽象\n",
    "\n",
    "        '''\n",
    "        三个连续的Inception模块组，三个Inception模块组中各自分别有多个Inception Module，这部分是Inception Module V3\n",
    "        的精华所在。每个Inception模块组内部的几个Inception Mdoule结构非常相似，但是存在一些细节的不同\n",
    "        '''\n",
    "        # Inception blocks\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], # 设置所有模块组的默认参数\n",
    "                            stride=1, padding='SAME'): # 将所有卷积层、最大池化、平均池化层步长都设置为1\n",
    "          # mixed: 35 x 35 x 256.\n",
    "          # 第一个模块组包含了三个结构类似的Inception Module\n",
    "              with tf.variable_scope('Mixed_5b'): # 第一个Inception Module名称。Inception Module有四个分支\n",
    "                with tf.variable_scope('Branch_0'): # 第一个分支64通道的1*1卷积\n",
    "                    branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                with tf.variable_scope('Branch_1'): # 第二个分支48通道1*1卷积，链接一个64通道的5*5卷积\n",
    "                    branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "                with tf.variable_scope('Branch_2'):\n",
    "                    branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                    branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "                with tf.variable_scope('Branch_3'): # 第四个分支为3*3的平均池化，连接32通道的1*1卷积\n",
    "                    branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                    branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3) # 将四个分支的输出合并在一起（第三个维度合并，即输出通道上合并）\n",
    "\n",
    "          '''\n",
    "          因为这里所有层步长均为1，并且padding模式为SAME，所以图片尺寸不会缩小，但是通道数增加了。四个分支通道数之和\n",
    "          64+64+96+32=256，最终输出的tensor的图片尺寸为35*35*256。\n",
    "          第一个模块组所有Inception Module输出图片尺寸都是35*35，但是后两个输出通道数会发生变化。\n",
    "          '''\n",
    "\n",
    "          # mixed_1: 35 x 35 x 288.\n",
    "        with tf.variable_scope('Mixed_5c'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv_1_0c_5x5')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "          # mixed_2: 35 x 35 x 288.\n",
    "          with tf.variable_scope('Mixed_5d'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "          # 第二个Inception模块组。第二个到第五个Inception Module结构相似。\n",
    "          # mixed_3: 17 x 17 x 768.\n",
    "        with tf.variable_scope('Mixed_6a'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 384, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_1x1') # 图片会被压缩\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_1x1') # 图片被压缩\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                         scope='MaxPool_1a_3x3')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2], 3) # 输出尺寸定格在17 x 17 x 768\n",
    "\n",
    "          # mixed4: 17 x 17 x 768.\n",
    "          with tf.variable_scope('Mixed_6b'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 128, [1, 7], scope='Conv2d_0b_1x7') # 串联1*7卷积和7*1卷积合成7*7卷积，减少了参数，减轻了过拟合\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'): \n",
    "                branch_2 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1') # 反复将7*7卷积拆分\n",
    "                branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0b_7x1') \n",
    "                branch_2 = slim.conv2d(branch_2, 128, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "          # mixed_5: 17 x 17 x 768.\n",
    "          with tf.variable_scope('Mixed_6c'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "              '''\n",
    "              我们的网络每经过一个inception module，即使输出尺寸不变，但是特征都相当于被重新精炼了一遍，\n",
    "              其中丰富的卷积和非线性化对提升网络性能帮助很大。\n",
    "              '''\n",
    "              branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "          # mixed_6: 17 x 17 x 768.\n",
    "          with tf.variable_scope('Mixed_6d'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "          # mixed_7: 17 x 17 x 768.\n",
    "          with tf.variable_scope('Mixed_6e'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "        end_points['Mixed_6e'] = net # 将Mixed_6e存储于end_points中，作为Auxiliary Classifier辅助模型的分类\n",
    "\n",
    "          # 第三个inception模块组包含了三个inception module\n",
    "          # mixed_8: 8 x 8 x 1280.\n",
    "         with tf.variable_scope('Mixed_7a'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_0 = slim.conv2d(branch_0, 320, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_3x3') # 压缩图片\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_3x3')\n",
    "            with tf.variable_scope('Branch_2'): # 池化层不会对输出通道数产生改变\n",
    "                branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                         scope='MaxPool_1a_3x3')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2], 3) # 输出图片尺寸被缩小，通道数增加，tensor的总size在持续下降中\n",
    "          # mixed_9: 8 x 8 x 2048.\n",
    "          with tf.variable_scope('Mixed_7b'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = tf.concat([\n",
    "                  slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "                  slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_0b_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(\n",
    "                  branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = tf.concat([\n",
    "                  slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'),\n",
    "                  slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(\n",
    "                  branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3) # 输出通道数增加到2048\n",
    "\n",
    "          # mixed_10: 8 x 8 x 2048.\n",
    "          with tf.variable_scope('Mixed_7c'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = tf.concat([\n",
    "                slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "                slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_0c_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(\n",
    "                branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = tf.concat([\n",
    "                slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'),\n",
    "                slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(\n",
    "                branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "        return net, end_points\n",
    "          #Inception V3网络的核心部分，即卷积层部分就完成了\n",
    "          '''\n",
    "          设计inception net的重要原则是图片尺寸不断缩小，inception模块组的目的都是将空间结构简化，同时将空间信息转化为\n",
    "          高阶抽象的特征信息，即将空间维度转为通道的维度。降低了计算量。Inception Module是通过组合比较简单的特征\n",
    "          抽象（分支1）、比较比较复杂的特征抽象（分支2和分支3）和一个简化结构的池化层（分支4），一共四种不同程度的\n",
    "          特征抽象和变换来有选择地保留不同层次的高阶特征，这样最大程度地丰富网络的表达能力。\n",
    "      '''\n",
    "\n",
    "\n",
    "########全局平均池化、Softmax和Auxiliary Logits########\n",
    "def inception_v3(inputs,\n",
    "                 num_classes=1000, # 最后需要分类的数量（比赛数据集的种类数）\n",
    "                 is_training=True, # 标志是否为训练过程，只有在训练时Batch normalization和Dropout才会启用\n",
    "                 dropout_keep_prob=0.8, # 节点保留比率\n",
    "                 prediction_fn=slim.softmax, # 最后用来分类的函数\n",
    "                 spatial_squeeze=True, # 参数标志是否对输出进行squeeze操作（去除维度数为1的维度，比如5*3*1转为5*3）\n",
    "                 reuse=None, # 是否对网络和Variable进行重复使用\n",
    "                 scope='InceptionV3'): # 包含函数默认参数的环境\n",
    "\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes], # 定义参数默认值\n",
    "                         reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], # 定义标志默认值\n",
    "                        is_training=is_training):\n",
    "      # 拿到最后一层的输出net和重要节点的字典表end_points\n",
    "          net, end_points = inception_v3_base(inputs, scope=scope) # 用定义好的函数构筑整个网络的卷积部分\n",
    "\n",
    "          # Auxiliary Head logits作为辅助分类的节点，对分类结果预测有很大帮助\n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                              stride=1, padding='SAME'): # 将卷积、最大池化、平均池化步长设置为1\n",
    "            aux_logits = end_points['Mixed_6e'] # 通过end_points取到Mixed_6e\n",
    "            with tf.variable_scope('AuxLogits'):\n",
    "                aux_logits = slim.avg_pool2d(\n",
    "                    aux_logits, [5, 5], stride=3, padding='VALID', # 在Mixed_6e之后接平均池化。压缩图像尺寸\n",
    "                    scope='AvgPool_1a_5x5')\n",
    "                aux_logits = slim.conv2d(aux_logits, 128, [1, 1], # 卷积。压缩图像尺寸。\n",
    "                                       scope='Conv2d_1b_1x1')\n",
    "\n",
    "              # Shape of feature map before the final layer.\n",
    "                aux_logits = slim.conv2d(\n",
    "                  aux_logits, 768, [5,5],\n",
    "                  weights_initializer=trunc_normal(0.01), # 权重初始化方式重设为标准差为0.01的正态分布\n",
    "                  padding='VALID', scope='Conv2d_2a_5x5')\n",
    "                aux_logits = slim.conv2d(\n",
    "                  aux_logits, num_classes, [1, 1], activation_fn=None,\n",
    "                  normalizer_fn=None, weights_initializer=trunc_normal(0.001), # 输出变为1*1*1000\n",
    "                  scope='Conv2d_2b_1x1')\n",
    "                if spatial_squeeze: # tf.squeeze消除tensor中前两个为1的维度。\n",
    "                    aux_logits = tf.squeeze(aux_logits, [1, 2], name='SpatialSqueeze')\n",
    "                end_points['AuxLogits'] = aux_logits # 最后将辅助分类节点的输出aux_logits储存到字典表end_points中\n",
    "\n",
    "          # 处理正常的分类预测逻辑\n",
    "          # Final pooling and prediction\n",
    "             with tf.variable_scope('Logits'):\n",
    "                net = slim.avg_pool2d(net, [8, 8], padding='VALID',\n",
    "                                      scope='AvgPool_1a_8x8')\n",
    "                # 1 x 1 x 2048\n",
    "                net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
    "                end_points['PreLogits'] = net\n",
    "                # 2048\n",
    "                logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, # 输出通道数1000\n",
    "                                     normalizer_fn=None, scope='Conv2d_1c_1x1') # 激活函数和规范化函数设为空\n",
    "            if spatial_squeeze: # tf.squeeze去除输出tensor中维度为1的节点\n",
    "                logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n",
    "            # 1000\n",
    "            end_points['Logits'] = logits\n",
    "            end_points['Predictions'] = prediction_fn(logits, scope='Predictions') # Softmax对结果进行分类预测\n",
    "    return logits, end_points # 最后返回logits和包含辅助节点的end_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet(object):\n",
    "\n",
    "    def __init__(self, hps, images, labels, mode):\n",
    "    \n",
    "        self.hps = hps\n",
    "        self._images = images\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "\n",
    "        self._extra_train_ops = []\n",
    "\n",
    "  # 构建模型图\n",
    "   def build_graph(self):\n",
    "    # 新建全局step\n",
    "    self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    # 构建ResNet网络模型\n",
    "    self._build_model()\n",
    "    # 构建优化训练操作\n",
    "    if self.mode == 'train':\n",
    "        self._build_train_op()\n",
    "    # 合并所有总结\n",
    "    self.summaries = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "  # 构建模型\n",
    "  def _build_model(self):\n",
    "    with tf.variable_scope('init'):\n",
    "        x = self._images\n",
    "        \"\"\"第一层卷积（3,3x3/1,16）\"\"\"\n",
    "        x = self._conv('init_conv', x, 3, 3, 16, self._stride_arr(1))\n",
    "\n",
    "    # 残差网络参数\n",
    "    strides = [1, 2, 2]\n",
    "    # 激活前置\n",
    "    activate_before_residual = [True, False, False]\n",
    "    if self.hps.use_bottleneck:\n",
    "      # bottleneck残差单元模块\n",
    "      res_func = self._bottleneck_residual\n",
    "      # 通道数量\n",
    "      filters = [16, 64, 128, 256]\n",
    "    else:\n",
    "      # 标准残差单元模块\n",
    "      res_func = self._residual\n",
    "      # 通道数量\n",
    "      filters = [16, 16, 32, 64]\n",
    "\n",
    "    # 第一组\n",
    "    with tf.variable_scope('unit_1_0'):\n",
    "        x = res_func(x, filters[0], filters[1], \n",
    "                   self._stride_arr(strides[0]),\n",
    "                   activate_before_residual[0])\n",
    "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
    "        with tf.variable_scope('unit_1_%d' % i):\n",
    "            x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n",
    "\n",
    "    # 第二组\n",
    "    with tf.variable_scope('unit_2_0'):\n",
    "        x = res_func(x, filters[1], filters[2], \n",
    "                   self._stride_arr(strides[1]),\n",
    "                   activate_before_residual[1])\n",
    "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
    "        with tf.variable_scope('unit_2_%d' % i):\n",
    "            x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n",
    "        \n",
    "    # 第三组\n",
    "    with tf.variable_scope('unit_3_0'):\n",
    "        x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n",
    "                   activate_before_residual[2])\n",
    "    for i in six.moves.range(1, self.hps.num_residual_units):\n",
    "        with tf.variable_scope('unit_3_%d' % i):\n",
    "            x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n",
    "\n",
    "    # 全局池化层\n",
    "    with tf.variable_scope('unit_last'):\n",
    "        x = self._batch_norm('final_bn', x)\n",
    "        x = self._relu(x, self.hps.relu_leakiness)\n",
    "        x = self._global_avg_pool(x)\n",
    "\n",
    "    # 全连接层 + Softmax\n",
    "    with tf.variable_scope('logit'):\n",
    "        logits = self._fully_connected(x, self.hps.num_classes)\n",
    "        self.predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    # 构建损失函数\n",
    "    with tf.variable_scope('costs'):\n",
    "      # 交叉熵\n",
    "      xent = tf.nn.softmax_cross_entropy_with_logits(\n",
    "          logits=logits, labels=self.labels)\n",
    "      # 加和\n",
    "      self.cost = tf.reduce_mean(xent, name='xent')\n",
    "      # L2正则，权重衰减\n",
    "      self.cost += self._decay()\n",
    "      # 添加cost总结，用于Tensorborad显示\n",
    "      tf.summary.scalar('cost', self.cost)\n",
    "\n",
    "  # 构建训练操作\n",
    "  def _build_train_op(self):\n",
    "    # 学习率/步长\n",
    "    self.lrn_rate = tf.constant(self.hps.lrn_rate, tf.float32)\n",
    "    tf.summary.scalar('learning_rate', self.lrn_rate)\n",
    "\n",
    "    # 计算训练参数的梯度\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    grads = tf.gradients(self.cost, trainable_variables)\n",
    "\n",
    "    # 设置优化方法\n",
    "    if self.hps.optimizer == 'sgd':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\n",
    "    elif self.hps.optimizer == 'mom':\n",
    "        optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9)\n",
    "\n",
    "    # 梯度优化操作\n",
    "    apply_op = optimizer.apply_gradients(\n",
    "                        zip(grads, trainable_variables),\n",
    "                        global_step=self.global_step, \n",
    "                        name='train_step')\n",
    "    \n",
    "    # 合并BN更新操作\n",
    "    train_ops = [apply_op] + self._extra_train_ops\n",
    "    # 建立优化操作组\n",
    "    self.train_op = tf.group(*train_ops)\n",
    "\n",
    "\n",
    "  # 把步长值转换成tf.nn.conv2d需要的步长数组\n",
    "  def _stride_arr(self, stride):    \n",
    "    return [1, stride, stride, 1]\n",
    "\n",
    "  # 残差单元模块\n",
    "  def _residual(self, x, in_filter, out_filter, stride, activate_before_residual=False):\n",
    "    # 是否前置激活(取残差直连之前进行BN和ReLU）\n",
    "    if activate_before_residual:\n",
    "      with tf.variable_scope('shared_activation'):\n",
    "        # 先做BN和ReLU激活\n",
    "        x = self._batch_norm('init_bn', x)\n",
    "        x = self._relu(x, self.hps.relu_leakiness)\n",
    "        # 获取残差直连\n",
    "        orig_x = x\n",
    "    else:\n",
    "      with tf.variable_scope('residual_only_activation'):\n",
    "        # 获取残差直连\n",
    "        orig_x = x\n",
    "        # 后做BN和ReLU激活\n",
    "        x = self._batch_norm('init_bn', x)\n",
    "        x = self._relu(x, self.hps.relu_leakiness)\n",
    "\n",
    "    # 第1子层\n",
    "    with tf.variable_scope('sub1'):\n",
    "      # 3x3卷积，使用输入步长，通道数(in_filter -> out_filter)\n",
    "      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n",
    "\n",
    "    # 第2子层\n",
    "    with tf.variable_scope('sub2'):\n",
    "      # BN和ReLU激活\n",
    "      x = self._batch_norm('bn2', x)\n",
    "      x = self._relu(x, self.hps.relu_leakiness)\n",
    "      # 3x3卷积，步长为1，通道数不变(out_filter)\n",
    "      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n",
    "    \n",
    "    # 合并残差层\n",
    "    with tf.variable_scope('sub_add'):\n",
    "      # 当通道数有变化时\n",
    "      if in_filter != out_filter:\n",
    "        # 均值池化，无补零\n",
    "        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n",
    "        # 通道补零(第4维前后对称补零)\n",
    "        orig_x = tf.pad(orig_x, \n",
    "                        [[0, 0], \n",
    "                         [0, 0], \n",
    "                         [0, 0],\n",
    "                         [(out_filter-in_filter)//2, (out_filter-in_filter)//2]\n",
    "                        ])\n",
    "      # 合并残差\n",
    "      x += orig_x\n",
    "\n",
    "    tf.logging.debug('image after unit %s', x.get_shape())\n",
    "    return x\n",
    "\n",
    "  # bottleneck残差单元模块\n",
    "  def _bottleneck_residual(self, x, in_filter, out_filter, stride,\n",
    "                           activate_before_residual=False):\n",
    "    # 是否前置激活(取残差直连之前进行BN和ReLU）\n",
    "    if activate_before_residual:\n",
    "      with tf.variable_scope('common_bn_relu'):\n",
    "        # 先做BN和ReLU激活\n",
    "        x = self._batch_norm('init_bn', x)\n",
    "        x = self._relu(x, self.hps.relu_leakiness)\n",
    "        # 获取残差直连\n",
    "        orig_x = x\n",
    "    else:\n",
    "      with tf.variable_scope('residual_bn_relu'):\n",
    "        # 获取残差直连\n",
    "        orig_x = x\n",
    "        # 后做BN和ReLU激活\n",
    "        x = self._batch_norm('init_bn', x)\n",
    "        x = self._relu(x, self.hps.relu_leakiness)\n",
    "\n",
    "    # 第1子层\n",
    "    with tf.variable_scope('sub1'):\n",
    "      # 1x1卷积，使用输入步长，通道数(in_filter -> out_filter/4)\n",
    "      x = self._conv('conv1', x, 1, in_filter, out_filter/4, stride)\n",
    "\n",
    "    # 第2子层\n",
    "    with tf.variable_scope('sub2'):\n",
    "      # BN和ReLU激活\n",
    "      x = self._batch_norm('bn2', x)\n",
    "      x = self._relu(x, self.hps.relu_leakiness)\n",
    "      # 3x3卷积，步长为1，通道数不变(out_filter/4)\n",
    "      x = self._conv('conv2', x, 3, out_filter/4, out_filter/4, [1, 1, 1, 1])\n",
    "\n",
    "    # 第3子层\n",
    "    with tf.variable_scope('sub3'):\n",
    "      # BN和ReLU激活\n",
    "      x = self._batch_norm('bn3', x)\n",
    "      x = self._relu(x, self.hps.relu_leakiness)\n",
    "      # 1x1卷积，步长为1，通道数不变(out_filter/4 -> out_filter)\n",
    "      x = self._conv('conv3', x, 1, out_filter/4, out_filter, [1, 1, 1, 1])\n",
    "\n",
    "    # 合并残差层\n",
    "    with tf.variable_scope('sub_add'):\n",
    "      # 当通道数有变化时\n",
    "      if in_filter != out_filter:\n",
    "        # 1x1卷积，使用输入步长，通道数(in_filter -> out_filter)\n",
    "        orig_x = self._conv('project', orig_x, 1, in_filter, out_filter, stride)\n",
    "      \n",
    "      # 合并残差\n",
    "      x += orig_x\n",
    "\n",
    "    tf.logging.info('image after unit %s', x.get_shape())\n",
    "    return x\n",
    "\n",
    "\n",
    "  # Batch Normalization批归一化\n",
    "  # ((x-mean)/var)*gamma+beta\n",
    "  def _batch_norm(self, name, x):\n",
    "    with tf.variable_scope(name):\n",
    "      # 输入通道维数\n",
    "      params_shape = [x.get_shape()[-1]]\n",
    "      # offset\n",
    "      beta = tf.get_variable('beta', \n",
    "                             params_shape, \n",
    "                             tf.float32,\n",
    "                             initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "      # scale\n",
    "      gamma = tf.get_variable('gamma', \n",
    "                              params_shape, \n",
    "                              tf.float32,\n",
    "                              initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "\n",
    "      if self.mode == 'train':\n",
    "        # 为每个通道计算均值、标准差\n",
    "        mean, variance = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "        # 新建或建立测试阶段使用的batch均值、标准差\n",
    "        moving_mean = tf.get_variable('moving_mean', \n",
    "                                      params_shape, tf.float32,\n",
    "                                      initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "                                      trainable=False)\n",
    "        moving_variance = tf.get_variable('moving_variance', \n",
    "                                          params_shape, tf.float32,\n",
    "                                          initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "                                          trainable=False)\n",
    "        # 添加batch均值和标准差的更新操作(滑动平均)\n",
    "        # moving_mean = moving_mean * decay + mean * (1 - decay)\n",
    "        # moving_variance = moving_variance * decay + variance * (1 - decay)\n",
    "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "                                                        moving_mean, mean, 0.9))\n",
    "        self._extra_train_ops.append(moving_averages.assign_moving_average(\n",
    "                                                        moving_variance, variance, 0.9))\n",
    "      else:\n",
    "        # 获取训练中积累的batch均值、标准差\n",
    "        mean = tf.get_variable('moving_mean', \n",
    "                               params_shape, tf.float32,\n",
    "                               initializer=tf.constant_initializer(0.0, tf.float32),\n",
    "                               trainable=False)\n",
    "        variance = tf.get_variable('moving_variance', \n",
    "                                   params_shape, tf.float32,\n",
    "                                   initializer=tf.constant_initializer(1.0, tf.float32),\n",
    "                                   trainable=False)\n",
    "        # 添加到直方图总结\n",
    "        tf.summary.histogram(mean.op.name, mean)\n",
    "        tf.summary.histogram(variance.op.name, variance)\n",
    "\n",
    "      # BN层：((x-mean)/var)*gamma+beta\n",
    "      y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 0.001)\n",
    "      y.set_shape(x.get_shape())\n",
    "      return y\n",
    "\n",
    "\n",
    "  # 权重衰减，L2正则loss\n",
    "  def _decay(self):\n",
    "    costs = []\n",
    "    # 遍历所有可训练变量\n",
    "    for var in tf.trainable_variables():\n",
    "      #只计算标有“DW”的变量\n",
    "      if var.op.name.find(r'DW') > 0:\n",
    "        costs.append(tf.nn.l2_loss(var))\n",
    "    # 加和，并乘以衰减因子\n",
    "    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n",
    "\n",
    "  # 2D卷积\n",
    "  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n",
    "    with tf.variable_scope(name):\n",
    "      n = filter_size * filter_size * out_filters\n",
    "      # 获取或新建卷积核，正态随机初始化\n",
    "      kernel = tf.get_variable(\n",
    "              'DW', \n",
    "              [filter_size, filter_size, in_filters, out_filters],\n",
    "              tf.float32, \n",
    "              initializer=tf.random_normal_initializer(stddev=np.sqrt(2.0/n)))\n",
    "      # 计算卷积\n",
    "      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\n",
    "\n",
    "  # leaky ReLU激活函数，泄漏参数leakiness为0就是标准ReLU\n",
    "  def _relu(self, x, leakiness=0.0):\n",
    "    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
    "  \n",
    "  # 全连接层，网络最后一层\n",
    "  def _fully_connected(self, x, out_dim):\n",
    "    # 输入转换成2D tensor，尺寸为[N,-1]\n",
    "    x = tf.reshape(x, [self.hps.batch_size, -1])\n",
    "    # 参数w，平均随机初始化，[-sqrt(3/dim), sqrt(3/dim)]*factor\n",
    "    w = tf.get_variable('DW', [x.get_shape()[1], out_dim],\n",
    "                        initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
    "    # 参数b，0值初始化\n",
    "    b = tf.get_variable('biases', [out_dim], initializer=tf.constant_initializer())\n",
    "    # 计算x*w+b\n",
    "    return tf.nn.xw_plus_b(x, w, b)\n",
    "\n",
    "  # 全局均值池化\n",
    "  def _global_avg_pool(self, x):\n",
    "    assert x.get_shape().ndims == 4\n",
    "    # 在第2&3维度上计算均值，尺寸由WxH收缩为1x1\n",
    "    return tf.reduce_mean(x, [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
