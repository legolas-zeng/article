{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例：用经典卷积神经网络对cifar-10数据进行图像分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##设定全局参数\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import CIFAR10.CIFAR_input as input\n",
    "FLAGS=tf.app.flags.FLAGS\n",
    "\n",
    "#模型参数\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64,\n",
    "                            \"\"\"Number of images to process in a batch.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_dir', './CIFAR10',\n",
    "                           \"\"\"Path to the CIFAR-10 data directory.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('use_fp16', False,\n",
    "                            \"\"\"Train the model using fp16.\"\"\")\n",
    "\n",
    "#全局变量\n",
    "IMAGE_SIZE=input.IMAGE_SIZE\n",
    "NUM_CLASSES=input.NUM_CLASSES\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN=input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL=input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL\n",
    "\n",
    "#训练过程中的常量\n",
    "MOVING_AVERAGE_DECAY=0.9999\n",
    "NUM_EPOCH_PER_DECAY=350.0 #epochs after which learning rate decays\n",
    "LEARNING_RATE_DECAY_FACTOR=0.1 #学习率衰减因子\n",
    "INITIAL_LEARNING_RATE=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##构建模型\n",
    "def inference(images):\n",
    "    \"\"\"\n",
    "    创建CIFAR-10模型\n",
    "    :param images: Images来自distorted_inputs()或inputs()\n",
    "    :return:\n",
    "    Logits神经元\n",
    "    \"\"\"\n",
    "    #conv1\n",
    "    with tf.variable_scope('conv1')as scope:\n",
    "        kernel=_variable_with_weight_decay('weights',shape=[5,5,3,64],stddev=5e-2,wd=0.0)\n",
    "        conv=tf.nn.conv2d(images,kernel,[1,1,1,1],padding='SAME')#卷积操作\n",
    "        biases=_variable_on_cpu('biases',[64],tf.constant_initializer(0.0))\n",
    "        pre_activation=tf.nn.bias_add(conv,biases)# WX+b\n",
    "        conv1=tf.nn.relu(pre_activation,name=scope.name)\n",
    "        _activation_summary(conv1)\n",
    "\n",
    "    #pool1\n",
    "    pool1=tf.nn.max_pool(conv1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME',name='pool1')\n",
    "\n",
    "\n",
    "    #norm1\n",
    "    norm1=tf.nn.lrn(pool1,4,bias=1.0,alpha=0.001/9.0,beta=0.75,name='norm1')\n",
    "\n",
    "    #conv2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        kernel=_variable_with_weight_decay('weights',shape=[5,5,64,64],stddev=5e-2,wd=0.0)\n",
    "        conv=tf.nn.conv2d(norm1,kernel,[1,1,1,1],padding='SAME')\n",
    "        biases=_variable_on_cpu('biases',[64],tf.constant_initializer(0.1))\n",
    "        pre_activation=tf.nn.bias_add(conv,biases)\n",
    "        conv2=tf.nn.relu(pre_activation,name=scope.name)\n",
    "        _activation_summary(conv2)\n",
    "\n",
    "     #norm2\n",
    "    norm2=tf.nn.lrn(conv2,4,bias=1.0,alpha=0.001/9.0,beta=0.75,name='norm2')\n",
    "\n",
    "    #pool2\n",
    "    pool2=tf.nn.max_pool(norm2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME',name='pool2')\n",
    "\n",
    "    #local3\n",
    "    with tf.variable_scope('local3')as scope:\n",
    "        #Move everything into depth so we can perform a single matrix multiply\n",
    "        reshape=tf.reshape(pool2,[FLAGS.batch_size,-1])\n",
    "        dim=reshape.get_shape()[1].value\n",
    "        weights=_variable_with_weight_decay('weights',shape=[dim,384],stddev=0.04,wd=0.004)\n",
    "        biases=_variable_on_cpu('biases',[384],tf.constant_initializer(0.1))\n",
    "        local3=tf.nn.relu(tf.matmul(reshape,weights)+biases,name=scope.name)\n",
    "        _activation_summary(local3)\n",
    "\n",
    "     #local4\n",
    "    with tf.variable_scope('local4') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                              stddev=0.04, wd=0.004)\n",
    "        biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "        local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "        _activation_summary(local4)\n",
    "\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights=_variable_with_weight_decay('weights',[192,NUM_CLASSES],stddev=1/192.0,wd=0.0)\n",
    "        biases=_variable_on_cpu('biases',[NUM_CLASSES],tf.constant_initializer(0.0))\n",
    "        softmax_linear=tf.add(tf.matmul(local4,weights),biases,name=scope.name)\n",
    "        _activation_summary(softmax_linear)\n",
    "\n",
    "    return softmax_linear \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_with_weight_decay(name,shape,stddev,wd):\n",
    "    \"\"\"\n",
    "    Helper to create an initialized Variable with weight decay\n",
    "\n",
    "    这里变量被初始化为截断正态分布\n",
    "    :param stddev:标准差\n",
    "    :param wd: add L2 loss weight decay multiplied by this float. If None, weight decay is not added for this Variable\n",
    "    :return:\n",
    "    Variable tensor\n",
    "    \"\"\"\n",
    "\n",
    "    dtype=tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "    var=_variable_on_cpu(name,shape,tf.truncated_normal_initializer(stddev=stddev,dtype=dtype))\n",
    "    if wd is not None:\n",
    "      weight_decay=tf.multiply(tf.nn.l2_loss(var),wd,name='weight_loss')\n",
    "      tf.add_to_collection('losses',weight_decay)\n",
    "\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _variable_on_cpu(name,shape,initializer):\n",
    "    \"\"\"\n",
    "    Helper to create a Variable stored oon CPU memory\n",
    "    :param name: 变量名\n",
    "    :param shape: lists of ints\n",
    "    :param initializer: 初始化变量值\n",
    "    :return:\n",
    "    Variable Tensor\n",
    "    \"\"\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        dtype=tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "        var=tf.get_variable(name,shape,initializer=initializer,dtype=dtype)\n",
    "        return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算loss\n",
    "def loss(logits,labels):\n",
    "    \"\"\"\n",
    "    Add L2loss to all the trainable variables\n",
    "    Add summary for \"loss\" and \"loss/avg\"\n",
    "    :param logits: logits from inference()\n",
    "    :param labels: labels from distorted_inputs or inputs() 1-D tensor of shape[batch_size]\n",
    "\n",
    "    :return: loss tensor of type float\n",
    "    \"\"\"\n",
    "\n",
    "    #计算平均交叉熵损失对一个batch\n",
    "    labels=tf.cast(labels,tf.int64)\n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name=\"cross_entropy_per_exapmle\")\n",
    "    cross_entropy_mean=tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "    tf.add_to_collection('losses',cross_entropy_mean)\n",
    "\n",
    "    #总共的损失应该是交叉熵损失加上权重衰减项（L2 LOSS）\n",
    "    #权重的二范数值刚刚也加到了'losses'的collection里，这里的tf.add_n()就是将loss和刚刚的weights的二范数值对应相加\n",
    "    return tf.add_n(tf.get_collection('losses'),name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 更新参数/train_op\n",
    "def train(total_loss,global_step):\n",
    "    \"\"\"\n",
    "    Train CIFAR-10 model\n",
    "    设立优化器，并对于所有可训练变量添加滑动平均\n",
    "    :param total_loss:Total loss from loss()\n",
    "    :param global_step:integer Varibale conunting the number of trainnig steps processed\n",
    "    :return: train_op:op for training\n",
    "    \"\"\"\n",
    "    #Variables that affect learning rate\n",
    "    num_batches_per_epoch=NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN/FLAGS.batch_size\n",
    "    decay_steps=int(num_batches_per_epoch* NUM_EPOCH_PER_DECAY)\n",
    "\n",
    "    #decay the learning rate exponentially based on the number of steps\n",
    "    #随着迭代过程衰减学习率\n",
    "    lr=tf.train.exponential_decay(\n",
    "        INITIAL_LEARNING_RATE,global_step,\n",
    "        decay_steps,LEARNING_RATE_DECAY_FACTOR,staircase=True)\n",
    "    tf.summary.scalar('learning_rate',lr)\n",
    "\n",
    "    #滑动平均 of all losses and associated summaries\n",
    "    loss_averages_op=_add_loss_summaries(total_loss)\n",
    "\n",
    "    #计算梯度\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt=tf.train.GradientDescentOptimizer(lr)\n",
    "        grads=opt.compute_gradients(total_loss)\n",
    "\n",
    "    #apply gradients\n",
    "    apply_gradient_op=opt.apply_gradients(grads,global_step=global_step)\n",
    "    #This is the second part of `minimize()`. It returns an `Operation` that applies gradients.\n",
    "\n",
    "    #add histogram\n",
    "    for grad,var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name+'/gradients',grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op\n",
    "\n",
    "#计算loss的平均值\n",
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"\n",
    "    Add summaries for losses in CIFAR-10 model\n",
    "    Generates moving average for all losses and \n",
    "    associated summaries of visualizing the performnce of the network\n",
    "    :param total_loss:Total loss from loss()\n",
    "    :return:\n",
    "    loss_averages_op: op for generating moving averages of losses\n",
    "    \"\"\"\n",
    "    #计算moving average of all individual losses and the total loss\n",
    "    #MovingAverage为滑动平均，计算方法：对于一个给定的数列，首先设定一个固定的值k，\n",
    "    #然后分别计算第1项到第k项，第2项到第k+1项，第3项到第k+2项的平均值，依次类推。\n",
    "    loss_averages=tf.train.ExponentialMovingAverage(0.9,name='avg')\n",
    "    losses=tf.get_collection('losses')\n",
    "    loss_averages_op=loss_averages.apply(losses+[total_loss])\n",
    "\n",
    "    #给每一个单独的losses和total loss attach a scalar summary;do the same\n",
    "    #for the averaged version of the losses\n",
    "    for l in losses+[total_loss]:\n",
    "        tf.summary.scalar(l.op.name+'(raw)',l)\n",
    "        tf.summary.scalar(l.op.name,loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练\n",
    "#全局参数\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from CIFAR10 import model_build\n",
    "FLAGS=tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir','E:/Python/tensorflow/CIFAR10',\"\"\"Directory\n",
    "where to write event logs and checkpoint\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps',100000,\"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('log_frequency', 10,\n",
    "                            \"\"\"How often to log results to the console.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train函数\n",
    "def train1():\n",
    "    with tf.Graph().as_default():\n",
    "        global_step=tf.contrib.framework.get_or_create_global_step()\n",
    "        #use the default graph in the process in the context\n",
    "        #global_step=tf.Variable(0,name='global_step',trainable=False)\n",
    "        #获取图像和标签\n",
    "        images,labels=model_build.distorted_inputs()\n",
    "\n",
    "        #创建一个图来计算神经元预测值，前向传播\n",
    "        logits=model_build.inference(images)\n",
    "\n",
    "        #计算loss\n",
    "        loss=model_build.loss(logits,labels)\n",
    "\n",
    "        #建一个图来来训练一个Batch的样本然后更新参数\n",
    "        train_op=model_build.train(loss,global_step)\n",
    "        #专门定义_LoggerHook类，在mon_sess这个对话中注册\n",
    "        class _LoggerHook(tf.train.SessionRunHook):\n",
    "            \"\"\"\n",
    "            Logs loss and runtime.\n",
    "            \"\"\"\n",
    "            def begin(self):\n",
    "                self._step=-1\n",
    "                self._start_time=time.time()\n",
    "\n",
    "            def before_run(self,run_context):\n",
    "                #Called before each call to run()\n",
    "                #返回‘SessionRunArgs’对象意味着ops或者tensors去加入即将到来的run()，\n",
    "                #这些ops和tensor回合之前的一起送入run()\n",
    "                #run()的参数里还可以包括你要feed的东西\n",
    "\n",
    "                #run_context参数包括了即将到来的run()的信息：原始的op和tensors\n",
    "                #当该函数运行完，图就确定了，就不能再加op了\n",
    "                self._step+=1\n",
    "                return tf.train.SessionRunArgs(loss) #Asks for loss value\n",
    "            def after_run(self,run_context,run_values):\n",
    "                #Called after eah call to run()\n",
    "                #'run value' argument contains results of requested ops/tensors by'before_run'\n",
    "                #the 'run_context' argument 与送入before_run的是一样的\n",
    "                #'run_context.request_stop()'can be called to stop the iteration\n",
    "                if self._step % FLAGS.log_frequency==0:#当取了FLAGS.log_frequency个batches的时候\n",
    "                    current_time=time.time()\n",
    "                    duration=current_time-self._start_time\n",
    "                    self._start_time=current_time\n",
    "\n",
    "                    loss_value=run_values.results\n",
    "                    examples_per_sec=FLAGS.log_frequency* FLAGS.batch_size/duration\n",
    "                    sec_per_barch=float(duration/FLAGS.log_frequency)\n",
    "                    format_str=('%s:step %d,loss=%.2f (%.1f examples/sec; %.3f' 'sec/batch')\n",
    "                    print(format_str %(datetime.now(),self._step,loss_value,examples_per_sec,sec_per_barch))\n",
    "\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "            #set proper session intializer/restorer,it also creates hooks related to\n",
    "            #checkpoint and summary saving\n",
    "            checkpoint_dir=FLAGS.train_dir,\n",
    "            hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),tf.train.NanTensorHook(loss),\n",
    "                   _LoggerHook()],\n",
    "            config=tf.ConfigProto(\n",
    "                log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n",
    "            while not mon_sess.should_stop():\n",
    "                mon_sess.run(train_op)\n",
    "                #此处表示在停止条件到达之前，循环运行train_op,更新模型参数\n",
    "\n",
    "def main(argv=None):\n",
    "            train1()\n",
    "\n",
    "if __name__=='__main__':\n",
    "            tf.app.run(main=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
