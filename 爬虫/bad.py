# -*-coding:utf-8 -*-import reimport stringimport randomimport sysimport osimport urllibimport urllib2from bs4 import BeautifulSoupimport requestsimport shutilimport timefrom lxml import etreeimport sysreload(sys)sys.setdefaultencoding('utf-8')user_id = 3822047450cookie = {"Cookie": "_T_WM=f80ea0f27d24fe885f1dfc4d45266728; SUB=_2A250xJG6DeRhGeNG61EW-SnLzjuIHXVURj_yrDV6PUJbkdBeLRPHkW1b6DMBJwmcYwdGSMdkGQ0MVEjFoQ..; SUHB=0k1yY0xVpbTUUp; SCF=Am0xQaueZmblp6GSsPX6Scv4YJvrqNPjr1euN6XMmqCULD3uJ3vWOU-9eGg35SDFR5q1FRFuHT8CEVQClTtMQ1g.; SSOLoginState=1505812970; H5_INDEX=3; H5_INDEX_TITLE=%E6%9F%8F%E6%A0%91%E4%B8%8B%E7%9A%84%E7%BE%8E%E7%94%B7%E5%AD%90"}url = 'http://weibo.cn/u/%d'%user_idurllist_set = set()word_count = 1image_count = 1result = ""r=requests.get(url,cookies=cookie)code=r.status_codeif code==200:	print u"id和cookie加载正确"else:	print u"id和cookie加载失败!!!"	exit(1)res = r.contentselector = etree.HTML(res)pageNum = selector.xpath('//*[@id="pagelist"]/form/div/input[1]')[0].get('value')print u"共计%s页需要爬取"%pageNumpageNum=int(pageNum)page_list=range(1,pageNum+1)for page in page_list:	url = 'http://weibo.cn/u/%d?page=%d' % (user_id, page)	ret = requests.get(url, cookies=cookie)	ret_code=ret.status_code	if ret_code ==200:		print u"第%s页获取成功"%page	else:		print u"第%s页获取失败"%page	res=ret.content	selector = etree.HTML(res)	content = selector.xpath('//span[@class="ctt"]')	for page_content in content:		text=page_content.xpath('string(.)')		content = text.replace('\n', '').replace(' ', '')		print content		content=content+"\n"		result=result+content		word_count+=1	soup = BeautifulSoup(res, "lxml")	oripic = soup.find_all('a',href=re.compile(r'^https://weibo.cn/mblog/oripic',re.S))	picAll = soup.find_all('a',href=re.compile(r'^https://weibo.cn/mblog/picAll',re.I))	for imgurl in oripic:		urllist_set.add(requests.get(imgurl['href'], cookies=cookie).url)		image_count+=1	for imgALL in picAll:		html_content = requests.get(imgALL['href'], cookies=cookie).content		soup = BeautifulSoup(html_content, "lxml")		urllist2 = soup.find_all('a', href=re.compile(r'^/mblog/oripic', re.I))		print urllist2		for imgurl in urllist2:			imgurl['href'] = 'http://weibo.cn' + imgurl['href']			urllist_set.add(requests.get(imgurl['href'], cookies=cookie).url)			image_count += 1	print u"第%s页图片分析完成"%page	time.sleep(10)word = open(os.getcwd()+"/%d"%user_id, "wb")word.write(result)word_path=os.getcwd()+'/%d'%user_idprint u'文字爬取完毕'link = ""img = open(os.getcwd()+"/%s_image"%user_id, "wb")for eachlink in urllist_set:    link = link + eachlink +"\n"img.write(link)print u'图片链接爬取完毕'if not urllist_set:    print u'该用户原创微博中不存在图片'else:    image_path=os.getcwd()+'/weibo_image'    if os.path.exists(image_path) is False:        os.mkdir(image_path)    x = 1    for imgurl in urllist_set:        temp= image_path + '/%s.jpg' % x        print u'正在下载第%s张图片' % x        try:            r = requests.get(imgurl, stream=True)            if r.status_code == 200:                with open(temp, 'wb') as f:                    r.raw.decode_content = True                    shutil.copyfileobj(r.raw, f)        except:            print u"该图片下载失败:%s"%imgurl        x += 1print u'原创微博爬取完毕，共%d条，保存路径%s'%(word_count - 3,word_path)print u'微博图片爬取完毕，共%d张，保存路径%s'%(image_count - 1,image_path)